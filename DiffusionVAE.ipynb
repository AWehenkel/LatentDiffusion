{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:tgh727hn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 68012<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value=' 2.71MB of 2.71MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/Users/awehenkel/Documents/Ecole/2020-2021/Research/LatentDiffusion/wandb/run-20210209_173355-tgh727hn/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/Users/awehenkel/Documents/Ecole/2020-2021/Research/LatentDiffusion/wandb/run-20210209_173355-tgh727hn/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train Loss</td><td>18.32189</td></tr><tr><td>_step</td><td>74</td></tr><tr><td>_runtime</td><td>1836</td></tr><tr><td>_timestamp</td><td>1612890275</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train Loss</td><td>█▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 76 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">likely-night-915</strong>: <a href=\"https://wandb.ai/awehenkel/latent_diffusion/runs/tgh727hn\" target=\"_blank\">https://wandb.ai/awehenkel/latent_diffusion/runs/tgh727hn</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:tgh727hn). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.18 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.17<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">copper-surf-925</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/awehenkel/latent_diffusion\" target=\"_blank\">https://wandb.ai/awehenkel/latent_diffusion</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/awehenkel/latent_diffusion/runs/ocsozrlx\" target=\"_blank\">https://wandb.ai/awehenkel/latent_diffusion/runs/ocsozrlx</a><br/>\n",
       "                Run data is saved locally in <code>/Users/awehenkel/Documents/Ecole/2020-2021/Research/LatentDiffusion/wandb/run-20210209_180529-ocsozrlx</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from Models import TemporalDecoder, TemporalEncoder, DataDiffuser, TransitionNet, SimpleImageDecoder, SimpleImageEncoder, PositionalEncoder, StupidPositionalEncoder\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def add_noise(x):\n",
    "    \"\"\"\n",
    "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
    "    \"\"\"\n",
    "    noise = x.new().resize_as_(x).uniform_()\n",
    "    x = x * 255 + noise\n",
    "    x = x / 256\n",
    "    return x\n",
    "\n",
    "def getMNISTDataLoader(bs):\n",
    "    # MNIST Dataset\n",
    "    train_dataset = datasets.MNIST(root='./mnist_data/', train=True, download=True, transform=transforms.Compose([\n",
    "                                      transforms.Resize((32, 32)),\n",
    "                                      #transforms.ToTensor(),\n",
    "                                      #add_noise,\n",
    "                                      ToTensor(),\n",
    "        AddUniformNoise()\n",
    "                                  ]))\n",
    "    test_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.Resize((32, 32)),\n",
    "                                      #transforms.ToTensor(),\n",
    "                                      #add_noise,\n",
    "                                      ToTensor(),\n",
    "                                      AddUniformNoise()\n",
    "                                  ]))\n",
    "\n",
    "    # Data Loader (Input Pipeline)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def getCIFAR10DataLoader(bs):\n",
    "    # MNIST Dataset\n",
    "    train_dataset = datasets.CIFAR10(root='./cifar10_data/', train=True, download=True, transform=transforms.Compose([\n",
    "                                      transforms.Resize(32),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      add_noise,\n",
    "                                      # transforms.ToTensor()\n",
    "                                  ]))\n",
    "    test_dataset = datasets.CIFAR10(root='./cifar10_data/', train=False, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.Resize(32),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      add_noise,\n",
    "                                      # transforms.ToTensor()\n",
    "                                  ]))\n",
    "\n",
    "    # Data Loader (Input Pipeline)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def logit(x, alpha=1E-6):\n",
    "    y = alpha + (1.-2*alpha)*x\n",
    "    return np.log(y) - np.log(1. - y)\n",
    "\n",
    "\n",
    "def logit_back(x, alpha=1E-6):\n",
    "    y = torch.sigmoid(x)\n",
    "    return (y - alpha)/(1.-2*alpha)\n",
    "\n",
    "\n",
    "class AddUniformNoise(object):\n",
    "    def __init__(self, alpha=1E-6):\n",
    "        self.alpha = alpha\n",
    "    def __call__(self,samples):\n",
    "        samples = np.array(samples,dtype = np.float32)\n",
    "        samples += np.random.uniform(size = samples.shape)\n",
    "        samples = logit(samples/256., self.alpha)\n",
    "        return samples\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self,samples):\n",
    "        samples = torch.from_numpy(np.array(samples,dtype = np.float32)).float()\n",
    "        return samples\n",
    "\n",
    "\n",
    "class CNNDiffusionModel(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CNNDiffusionModel, self).__init__()\n",
    "        self.T_MAX = kwargs['T_MAX']\n",
    "        self.latent_s = kwargs['latent_s']\n",
    "        self.t_emb_s = kwargs['t_emb_s']\n",
    "        self.CNN = kwargs['CNN']\n",
    "        self.register_buffer(\"beta_min\", torch.tensor(kwargs['beta_min']))\n",
    "        self.register_buffer(\"beta_max\", torch.tensor(kwargs['beta_max']))\n",
    "        self.device = 'cpu'\n",
    "        self.img_size = [1, 32, 32]\n",
    "        self.pos_enc = PositionalEncoder(self.t_emb_s // 2)  # StupidPositionalEncoder(T_MAX)  #\n",
    "        self.simplified_trans = kwargs['simplified_trans']\n",
    "\n",
    "        enc_net = [kwargs['enc_w']] * kwargs['enc_l']\n",
    "        dec_net = [kwargs['dec_w']] * kwargs['dec_l']\n",
    "        trans_net = [kwargs['trans_w']] * kwargs['trans_l']\n",
    "\n",
    "        if self.CNN:\n",
    "            self.enc = SimpleImageEncoder(self.img_size, self.latent_s, enc_net, t_dim=self.t_emb_s).to(dev)\n",
    "            self.dec = SimpleImageDecoder(self.enc.features_dim, self.latent_s, dec_net, t_dim=self.t_emb_s,\n",
    "                                          out_c=self.img_size[0]).to(dev)\n",
    "        else:\n",
    "            self.dec = TemporalDecoder(32**2, self.latent_s, dec_net, self.t_emb_s).to(dev)\n",
    "            self.enc = TemporalEncoder(32**2, self.latent_s, enc_net, self.t_emb_s).to(dev)\n",
    "\n",
    "        self.trans = TransitionNet(self.latent_s, trans_net, self.t_emb_s).to(dev)\n",
    "        self.dif = DataDiffuser(beta_min=self.beta_min, beta_max=self.beta_max, t_max=self.T_MAX).to(dev)\n",
    "        self.sampling_t0 = False\n",
    "\n",
    "    def loss(self, x0):\n",
    "        if self.sampling_t0:\n",
    "            t0 = torch.randint(0, self.T_MAX - 1, [x0.shape[0]]).to(dev)\n",
    "            x_t0, sigma_x_t0 = self.dif.diffuse(x0, t0, torch.zeros(x0.shape[0]).long().to(dev))\n",
    "        else:\n",
    "            t0 = torch.zeros(x0.shape[0]).to(dev).long()\n",
    "            x_t0 = x0\n",
    "\n",
    "        z_t0 = self.enc(x_t0.view(-1, *self.img_size), self.pos_enc(t0.float().unsqueeze(1)))\n",
    "        # z_t0 = z_t0 + torch.randn(z_t0.shape).to(dev) * (1 - dif.alphas[t0]).sqrt().unsqueeze(1).expand(-1, z_t0.shape[1])\n",
    "        t = torch.torch.distributions.Uniform(t0.float() + 1, torch.ones_like(t0) * self.T_MAX).sample().long().to(dev)\n",
    "\n",
    "        z_t, sigma_z = self.dif.diffuse(z_t0, t, t0)\n",
    "        x_t, sigma_x = self.dif.diffuse(x_t0, t, t0)\n",
    "\n",
    "        mu_x_pred = self.dec(z_t, self.pos_enc(t.float().unsqueeze(1)))\n",
    "        KL_x = ((mu_x_pred - x_t.view(bs, *self.img_size)) ** 2).view(bs, -1).sum(1) / sigma_x ** 2\n",
    "\n",
    "        if self.simplified_trans:\n",
    "            alpha_bar_t = self.dif.alphas[t].unsqueeze(1)#.expand(-1, self.latent_s)\n",
    "            alpha_t = self.dif.alphas_t[t].unsqueeze(1)#.expand(-1, self.latent_s)\n",
    "            beta_t = self.dif.betas[t].unsqueeze(1)#.expand(-1, self.latent_s)\n",
    "\n",
    "            mu_z_pred = (z_t - beta_t/(1-alpha_bar_t).sqrt() * self.trans(z_t, self.pos_enc(t.float().unsqueeze(1))))/alpha_t.sqrt()\n",
    "        else:\n",
    "            mu_z_pred = self.trans(z_t, self.pos_enc(t.float().unsqueeze(1)))\n",
    "        mu, sigma = self.dif.prev_mean(z_t0, z_t, t)\n",
    "\n",
    "        KL_z = ((mu - mu_z_pred) ** 2).sum(1) / sigma ** 2\n",
    "\n",
    "        loss = KL_x.mean(0) + KL_z.mean(0)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    def sample(self, nb_samples=1):\n",
    "        zT = torch.randn(64, self.latent_s).to(self.device)\n",
    "        z_t = zT\n",
    "        for t in range(self.T_MAX - 1, 0, -1):\n",
    "            t_t = torch.ones(64, 1).to(self.device) * t\n",
    "            if t > 0:\n",
    "                sigma = ((1 - self.dif.alphas[t - 1]) / (1 - self.dif.alphas[t]) * self.dif.betas[t]).sqrt()\n",
    "            else:\n",
    "                sigma = 0\n",
    "            if self.simplified_trans:\n",
    "                alpha_bar_t = self.dif.alphas[t]\n",
    "                alpha_t = self.dif.alphas_t[t]\n",
    "                beta_t = self.dif.betas[t]\n",
    "                mu_z_pred = (z_t - beta_t / (1 - alpha_bar_t).sqrt() * self.trans(z_t, self.pos_enc(t_t))) / alpha_t.sqrt()\n",
    "            else:\n",
    "                mu_z_pred = self.trans(z_t, self.pos_enc(t_t))\n",
    "            z_t = mu_z_pred + torch.randn(z_t.shape, device=self.device) * sigma\n",
    "\n",
    "        x_0 = self.dec(z_t, self.pos_enc(torch.zeros((nb_samples, 1), device=self.device))).view(nb_samples, -1)\n",
    "\n",
    "        return x_0\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"latent_diffusion\", entity=\"awehenkel\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bs = 100\n",
    "    config = {\n",
    "        'data': 'MNIST',\n",
    "        'T_MAX': 20,\n",
    "        'latent_s': 60,\n",
    "        't_emb_s': 30,\n",
    "        'CNN': False,\n",
    "        'enc_w': 300,\n",
    "        'enc_l': 4,\n",
    "        'dec_w': 200,\n",
    "        'dec_l': 3,\n",
    "        'trans_w': 200,\n",
    "        'trans_l': 3,\n",
    "        \"beta_min\": 1e-4,\n",
    "        \"beta_max\": .95,\n",
    "        'simplified_trans': True\n",
    "    }\n",
    "    wandb.config.update(config)\n",
    "    train_loader, test_loader = getMNISTDataLoader(bs)\n",
    "    img_size = [1, 32, 32]\n",
    "\n",
    "    # Compute Mean abd std per pixel\n",
    "    x_mean = 0\n",
    "    x_mean2 = 0\n",
    "    for batch_idx, (cur_x, target) in enumerate(train_loader):\n",
    "        cur_x = cur_x.view(bs, -1).float()\n",
    "        x_mean += cur_x.mean(0)\n",
    "        x_mean2 += (cur_x ** 2).mean(0)\n",
    "    x_mean /= batch_idx + 1\n",
    "    x_std = (x_mean2 / (batch_idx + 1) - x_mean ** 2) ** .5\n",
    "    x_std[x_std == 0.] = 1.\n",
    "\n",
    "    dev = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    model = CNNDiffusionModel(**config).to(dev)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, threshold=0.001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)\n",
    "\n",
    "    wandb.watch(model)\n",
    "    def get_X_back(x):\n",
    "        nb_x = x.shape[0]\n",
    "        x = x * x_std.to(dev).unsqueeze(0).expand(nb_x, -1) + x_mean.to(dev).unsqueeze(0).expand(nb_x, -1)\n",
    "        return logit_back(x)\n",
    "\n",
    "\n",
    "    #sample = list(train_loader)[0][0][[0]].expand(bs, -1, -1, -1)\n",
    "    #save_image(get_X_back(sample.to(dev)[[0]].reshape(1, -1)).reshape(1, 3, 32, 32), './Samples/Generated/sample_rel_' + '.png')\n",
    "    def train(epoch):\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            #data = sample\n",
    "            x0 = data.view(data.shape[0], -1).to(dev)\n",
    "\n",
    "            x0 = (x0 - x_mean.to(dev).unsqueeze(0).expand(bs, -1)) / x_std.to(dev).unsqueeze(0).expand(bs, -1)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = model.loss(x0)\n",
    "\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "        samples = get_X_back(model.sample(64)).view(64, *img_size)\n",
    "        save_image(samples, './Samples/Generated/sample_gen_' + str(epoch) + '.png')\n",
    "        scheduler.step(train_loss)\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "        wandb.log({\"Train Loss\": train_loss / len(train_loader.dataset), \"Samples\": [wandb.Image(samples)]})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 23.727632\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 21.072388\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 16.629475\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 26.206155\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 30.312065\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 19.416949\n",
      "====> Epoch: 0 Average loss: 26.5507\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 26.710417\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 17.572201\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 22.885115\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 26.090857\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 22.640598\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 22.536929\n",
      "====> Epoch: 1 Average loss: 24.3633\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 24.499421\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 18.362788\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 27.701003\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 26.103669\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 24.710042\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 28.645222\n",
      "====> Epoch: 2 Average loss: 23.4536\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 23.052664\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 29.100830\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 24.453262\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 20.008187\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 18.388997\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 24.076328\n",
      "====> Epoch: 3 Average loss: 23.3545\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 22.372974\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 22.569937\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 23.185117\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 26.605149\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 17.448276\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 24.836323\n",
      "====> Epoch: 4 Average loss: 22.6123\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 23.778550\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 21.908086\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 20.362493\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 25.931426\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 23.199600\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 21.812246\n",
      "====> Epoch: 5 Average loss: 22.6356\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 23.445994\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 23.066719\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 21.088430\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 24.252686\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 23.876282\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 21.785710\n",
      "====> Epoch: 6 Average loss: 22.6136\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 23.397026\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 18.870397\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 25.780244\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 20.598796\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 19.363187\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 20.319573\n",
      "====> Epoch: 7 Average loss: 22.4249\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 20.940427\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 19.426608\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 27.612446\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 19.789250\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 21.199534\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 17.282513\n",
      "====> Epoch: 8 Average loss: 22.1911\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 25.877639\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 25.039353\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 18.400023\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 17.052627\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 15.268407\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 20.553621\n",
      "====> Epoch: 9 Average loss: 22.2719\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 22.021294\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 25.044778\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 27.416606\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 23.475979\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 21.912317\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 22.192700\n",
      "====> Epoch: 10 Average loss: 22.0822\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 24.131409\n",
      "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 23.255593\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 17.978608\n",
      "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 26.209873\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 24.024639\n",
      "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 27.371882\n",
      "====> Epoch: 11 Average loss: 21.9481\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 20.731233\n",
      "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 19.912026\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 20.509290\n",
      "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 27.812058\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 29.806077\n",
      "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 20.754519\n",
      "====> Epoch: 12 Average loss: 22.0816\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 21.966189\n",
      "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 23.176250\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 26.345200\n",
      "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 21.822158\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 18.870449\n",
      "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 16.343702\n",
      "====> Epoch: 13 Average loss: 22.0170\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 27.321577\n",
      "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 21.572871\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 21.349822\n",
      "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 22.926199\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 14.852576\n",
      "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 22.168428\n",
      "====> Epoch: 14 Average loss: 21.6598\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 17.978094\n",
      "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 19.009012\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 20.377646\n",
      "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 21.686965\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 22.470930\n",
      "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 24.077229\n",
      "====> Epoch: 15 Average loss: 22.0272\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 21.576812\n",
      "Train Epoch: 16 [10000/60000 (17%)]\tLoss: 15.994552\n",
      "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 17.011107\n",
      "Train Epoch: 16 [30000/60000 (50%)]\tLoss: 22.994756\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 21.865676\n",
      "Train Epoch: 16 [50000/60000 (83%)]\tLoss: 20.600542\n",
      "====> Epoch: 16 Average loss: 21.8616\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 17.634338\n",
      "Train Epoch: 17 [10000/60000 (17%)]\tLoss: 18.357225\n",
      "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 19.326746\n",
      "Train Epoch: 17 [30000/60000 (50%)]\tLoss: 24.310505\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 15.518778\n",
      "Train Epoch: 17 [50000/60000 (83%)]\tLoss: 24.146658\n",
      "====> Epoch: 17 Average loss: 21.8325\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 20.007811\n",
      "Train Epoch: 18 [10000/60000 (17%)]\tLoss: 20.116423\n",
      "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 16.707393\n",
      "Train Epoch: 18 [30000/60000 (50%)]\tLoss: 20.158867\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 19.494635\n",
      "Train Epoch: 18 [50000/60000 (83%)]\tLoss: 19.145336\n",
      "====> Epoch: 18 Average loss: 21.8477\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 22.691746\n",
      "Train Epoch: 19 [10000/60000 (17%)]\tLoss: 17.196002\n",
      "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 25.567344\n",
      "Train Epoch: 19 [30000/60000 (50%)]\tLoss: 19.885386\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 18.110914\n",
      "Train Epoch: 19 [50000/60000 (83%)]\tLoss: 21.773477\n",
      "====> Epoch: 19 Average loss: 21.6802\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 22.942852\n",
      "Train Epoch: 20 [10000/60000 (17%)]\tLoss: 24.113311\n",
      "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 22.298594\n",
      "Train Epoch: 20 [30000/60000 (50%)]\tLoss: 23.234636\n",
      "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 21.722292\n",
      "Train Epoch: 20 [50000/60000 (83%)]\tLoss: 24.027334\n",
      "====> Epoch: 20 Average loss: 21.7897\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 19.161260\n",
      "Train Epoch: 21 [10000/60000 (17%)]\tLoss: 28.319092\n",
      "Train Epoch: 21 [20000/60000 (33%)]\tLoss: 20.081897\n",
      "Train Epoch: 21 [30000/60000 (50%)]\tLoss: 25.842263\n",
      "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 22.047681\n",
      "Train Epoch: 21 [50000/60000 (83%)]\tLoss: 18.757252\n",
      "====> Epoch: 21 Average loss: 21.5523\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 22.991067\n",
      "Train Epoch: 22 [10000/60000 (17%)]\tLoss: 21.993958\n",
      "Train Epoch: 22 [20000/60000 (33%)]\tLoss: 21.024131\n",
      "Train Epoch: 22 [30000/60000 (50%)]\tLoss: 23.957371\n",
      "Train Epoch: 22 [40000/60000 (67%)]\tLoss: 24.333660\n",
      "Train Epoch: 22 [50000/60000 (83%)]\tLoss: 21.974419\n",
      "====> Epoch: 22 Average loss: 21.7817\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 22.653994\n",
      "Train Epoch: 23 [10000/60000 (17%)]\tLoss: 24.907395\n",
      "Train Epoch: 23 [20000/60000 (33%)]\tLoss: 18.653405\n",
      "Train Epoch: 23 [30000/60000 (50%)]\tLoss: 20.889875\n",
      "Train Epoch: 23 [40000/60000 (67%)]\tLoss: 23.264849\n",
      "Train Epoch: 23 [50000/60000 (83%)]\tLoss: 17.712815\n",
      "====> Epoch: 23 Average loss: 21.5163\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 18.748513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24 [10000/60000 (17%)]\tLoss: 14.946508\n",
      "Train Epoch: 24 [20000/60000 (33%)]\tLoss: 22.190383\n",
      "Train Epoch: 24 [30000/60000 (50%)]\tLoss: 20.186127\n",
      "Train Epoch: 24 [40000/60000 (67%)]\tLoss: 21.021582\n",
      "Train Epoch: 24 [50000/60000 (83%)]\tLoss: 17.498707\n",
      "====> Epoch: 24 Average loss: 21.6126\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 22.227495\n",
      "Train Epoch: 25 [10000/60000 (17%)]\tLoss: 19.794542\n",
      "Train Epoch: 25 [20000/60000 (33%)]\tLoss: 25.325479\n",
      "Train Epoch: 25 [30000/60000 (50%)]\tLoss: 22.592842\n",
      "Train Epoch: 25 [40000/60000 (67%)]\tLoss: 22.368914\n",
      "Train Epoch: 25 [50000/60000 (83%)]\tLoss: 21.435010\n",
      "====> Epoch: 25 Average loss: 21.5007\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 16.931986\n",
      "Train Epoch: 26 [10000/60000 (17%)]\tLoss: 27.046577\n",
      "Train Epoch: 26 [20000/60000 (33%)]\tLoss: 21.111221\n",
      "Train Epoch: 26 [30000/60000 (50%)]\tLoss: 19.710983\n",
      "Train Epoch: 26 [40000/60000 (67%)]\tLoss: 23.683840\n",
      "Train Epoch: 26 [50000/60000 (83%)]\tLoss: 19.690667\n",
      "====> Epoch: 26 Average loss: 21.6930\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 20.836750\n",
      "Train Epoch: 27 [10000/60000 (17%)]\tLoss: 22.268081\n",
      "Train Epoch: 27 [20000/60000 (33%)]\tLoss: 22.303477\n",
      "Train Epoch: 27 [30000/60000 (50%)]\tLoss: 21.939141\n",
      "Train Epoch: 27 [40000/60000 (67%)]\tLoss: 20.277441\n",
      "Train Epoch: 27 [50000/60000 (83%)]\tLoss: 20.133369\n",
      "====> Epoch: 27 Average loss: 21.4504\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 22.002891\n",
      "Train Epoch: 28 [10000/60000 (17%)]\tLoss: 23.679631\n",
      "Train Epoch: 28 [20000/60000 (33%)]\tLoss: 21.908174\n",
      "Train Epoch: 28 [30000/60000 (50%)]\tLoss: 19.822667\n",
      "Train Epoch: 28 [40000/60000 (67%)]\tLoss: 23.843318\n",
      "Train Epoch: 28 [50000/60000 (83%)]\tLoss: 22.446985\n",
      "====> Epoch: 28 Average loss: 21.7593\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 20.692153\n",
      "Train Epoch: 29 [10000/60000 (17%)]\tLoss: 19.631553\n",
      "Train Epoch: 29 [20000/60000 (33%)]\tLoss: 20.724839\n",
      "Train Epoch: 29 [30000/60000 (50%)]\tLoss: 23.484885\n",
      "Train Epoch: 29 [40000/60000 (67%)]\tLoss: 20.970042\n",
      "Train Epoch: 29 [50000/60000 (83%)]\tLoss: 23.157087\n",
      "====> Epoch: 29 Average loss: 21.3713\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 15.952750\n",
      "Train Epoch: 30 [10000/60000 (17%)]\tLoss: 19.182870\n",
      "Train Epoch: 30 [20000/60000 (33%)]\tLoss: 24.340657\n",
      "Train Epoch: 30 [30000/60000 (50%)]\tLoss: 25.569192\n",
      "Train Epoch: 30 [40000/60000 (67%)]\tLoss: 21.404607\n",
      "Train Epoch: 30 [50000/60000 (83%)]\tLoss: 20.125698\n",
      "====> Epoch: 30 Average loss: 21.6203\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 20.999778\n",
      "Train Epoch: 31 [10000/60000 (17%)]\tLoss: 23.504773\n",
      "Train Epoch: 31 [20000/60000 (33%)]\tLoss: 22.553965\n",
      "Train Epoch: 31 [30000/60000 (50%)]\tLoss: 21.186104\n",
      "Train Epoch: 31 [40000/60000 (67%)]\tLoss: 20.287620\n",
      "Train Epoch: 31 [50000/60000 (83%)]\tLoss: 20.829854\n",
      "====> Epoch: 31 Average loss: 21.3668\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 22.250073\n",
      "Train Epoch: 32 [10000/60000 (17%)]\tLoss: 18.729990\n",
      "Train Epoch: 32 [20000/60000 (33%)]\tLoss: 22.294387\n",
      "Train Epoch: 32 [30000/60000 (50%)]\tLoss: 24.286477\n",
      "Train Epoch: 32 [40000/60000 (67%)]\tLoss: 18.036038\n",
      "Train Epoch: 32 [50000/60000 (83%)]\tLoss: 22.779880\n",
      "====> Epoch: 32 Average loss: 21.7013\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 27.629604\n",
      "Train Epoch: 33 [10000/60000 (17%)]\tLoss: 22.988071\n",
      "Train Epoch: 33 [20000/60000 (33%)]\tLoss: 21.952654\n",
      "Train Epoch: 33 [30000/60000 (50%)]\tLoss: 26.473452\n",
      "Train Epoch: 33 [40000/60000 (67%)]\tLoss: 28.408274\n",
      "Train Epoch: 33 [50000/60000 (83%)]\tLoss: 22.065874\n",
      "====> Epoch: 33 Average loss: 21.3850\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 19.689187\n",
      "Train Epoch: 34 [10000/60000 (17%)]\tLoss: 18.184813\n",
      "Train Epoch: 34 [20000/60000 (33%)]\tLoss: 19.104164\n",
      "Train Epoch: 34 [30000/60000 (50%)]\tLoss: 21.152693\n",
      "Train Epoch: 34 [40000/60000 (67%)]\tLoss: 18.482643\n",
      "Train Epoch: 34 [50000/60000 (83%)]\tLoss: 19.006664\n",
      "====> Epoch: 34 Average loss: 21.4176\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 21.995906\n",
      "Train Epoch: 35 [10000/60000 (17%)]\tLoss: 22.851082\n",
      "Train Epoch: 35 [20000/60000 (33%)]\tLoss: 22.317065\n",
      "Train Epoch: 35 [30000/60000 (50%)]\tLoss: 22.861895\n",
      "Train Epoch: 35 [40000/60000 (67%)]\tLoss: 23.904844\n",
      "Train Epoch: 35 [50000/60000 (83%)]\tLoss: 25.100381\n",
      "====> Epoch: 35 Average loss: 21.2590\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 15.811433\n",
      "Train Epoch: 36 [10000/60000 (17%)]\tLoss: 22.463723\n",
      "Train Epoch: 36 [20000/60000 (33%)]\tLoss: 20.214324\n",
      "Train Epoch: 36 [30000/60000 (50%)]\tLoss: 29.320684\n",
      "Train Epoch: 36 [40000/60000 (67%)]\tLoss: 18.817679\n",
      "Train Epoch: 36 [50000/60000 (83%)]\tLoss: 20.267657\n",
      "====> Epoch: 36 Average loss: 21.4432\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 23.579329\n",
      "Train Epoch: 37 [10000/60000 (17%)]\tLoss: 21.795679\n",
      "Train Epoch: 37 [20000/60000 (33%)]\tLoss: 21.907056\n",
      "Train Epoch: 37 [30000/60000 (50%)]\tLoss: 22.173450\n",
      "Train Epoch: 37 [40000/60000 (67%)]\tLoss: 21.660015\n",
      "Train Epoch: 37 [50000/60000 (83%)]\tLoss: 23.355051\n",
      "====> Epoch: 37 Average loss: 21.5564\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 19.767847\n",
      "Train Epoch: 38 [10000/60000 (17%)]\tLoss: 22.153704\n",
      "Train Epoch: 38 [20000/60000 (33%)]\tLoss: 17.146561\n",
      "Train Epoch: 38 [30000/60000 (50%)]\tLoss: 19.795928\n",
      "Train Epoch: 38 [40000/60000 (67%)]\tLoss: 19.479323\n",
      "Train Epoch: 38 [50000/60000 (83%)]\tLoss: 18.330055\n",
      "====> Epoch: 38 Average loss: 21.2739\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 21.099333\n",
      "Train Epoch: 39 [10000/60000 (17%)]\tLoss: 17.299917\n",
      "Train Epoch: 39 [20000/60000 (33%)]\tLoss: 19.377495\n",
      "Train Epoch: 39 [30000/60000 (50%)]\tLoss: 21.703892\n",
      "Train Epoch: 39 [40000/60000 (67%)]\tLoss: 23.852605\n",
      "Train Epoch: 39 [50000/60000 (83%)]\tLoss: 21.148276\n",
      "====> Epoch: 39 Average loss: 21.2656\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 23.647554\n",
      "Train Epoch: 40 [10000/60000 (17%)]\tLoss: 23.584233\n",
      "Train Epoch: 40 [20000/60000 (33%)]\tLoss: 19.181597\n",
      "Train Epoch: 40 [30000/60000 (50%)]\tLoss: 22.864839\n",
      "Train Epoch: 40 [40000/60000 (67%)]\tLoss: 17.870585\n",
      "Train Epoch: 40 [50000/60000 (83%)]\tLoss: 24.831335\n",
      "====> Epoch: 40 Average loss: 21.1435\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 25.466233\n",
      "Train Epoch: 41 [10000/60000 (17%)]\tLoss: 18.134781\n",
      "Train Epoch: 41 [20000/60000 (33%)]\tLoss: 19.552412\n",
      "Train Epoch: 41 [30000/60000 (50%)]\tLoss: 18.557607\n",
      "Train Epoch: 41 [40000/60000 (67%)]\tLoss: 20.173154\n",
      "Train Epoch: 41 [50000/60000 (83%)]\tLoss: 20.334563\n",
      "====> Epoch: 41 Average loss: 21.1821\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 25.114656\n",
      "Train Epoch: 42 [10000/60000 (17%)]\tLoss: 20.181951\n",
      "Train Epoch: 42 [20000/60000 (33%)]\tLoss: 17.368317\n",
      "Train Epoch: 42 [30000/60000 (50%)]\tLoss: 19.616079\n",
      "Train Epoch: 42 [40000/60000 (67%)]\tLoss: 22.067800\n",
      "Train Epoch: 42 [50000/60000 (83%)]\tLoss: 23.697573\n",
      "====> Epoch: 42 Average loss: 21.2217\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 27.930476\n",
      "Train Epoch: 43 [10000/60000 (17%)]\tLoss: 20.424862\n",
      "Train Epoch: 43 [20000/60000 (33%)]\tLoss: 22.361338\n",
      "Train Epoch: 43 [30000/60000 (50%)]\tLoss: 23.924307\n",
      "Train Epoch: 43 [40000/60000 (67%)]\tLoss: 21.978120\n",
      "Train Epoch: 43 [50000/60000 (83%)]\tLoss: 18.865948\n",
      "====> Epoch: 43 Average loss: 21.1919\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 24.152415\n",
      "Train Epoch: 44 [10000/60000 (17%)]\tLoss: 17.941029\n",
      "Train Epoch: 44 [20000/60000 (33%)]\tLoss: 19.163458\n",
      "Train Epoch: 44 [30000/60000 (50%)]\tLoss: 19.455919\n",
      "Train Epoch: 44 [40000/60000 (67%)]\tLoss: 17.948655\n",
      "Train Epoch: 44 [50000/60000 (83%)]\tLoss: 20.706838\n",
      "====> Epoch: 44 Average loss: 21.2158\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 18.272875\n",
      "Train Epoch: 45 [10000/60000 (17%)]\tLoss: 22.924236\n",
      "Train Epoch: 45 [20000/60000 (33%)]\tLoss: 19.822032\n",
      "Train Epoch: 45 [30000/60000 (50%)]\tLoss: 19.601405\n",
      "Train Epoch: 45 [40000/60000 (67%)]\tLoss: 18.240313\n",
      "Train Epoch: 45 [50000/60000 (83%)]\tLoss: 22.353379\n",
      "====> Epoch: 45 Average loss: 21.2879\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 19.927114\n",
      "Train Epoch: 46 [10000/60000 (17%)]\tLoss: 20.075154\n",
      "Train Epoch: 46 [20000/60000 (33%)]\tLoss: 20.865024\n",
      "Train Epoch: 46 [30000/60000 (50%)]\tLoss: 22.612253\n",
      "Train Epoch: 46 [40000/60000 (67%)]\tLoss: 20.836550\n",
      "Train Epoch: 46 [50000/60000 (83%)]\tLoss: 17.004307\n",
      "====> Epoch: 46 Average loss: 21.4879\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 23.483340\n",
      "Train Epoch: 47 [10000/60000 (17%)]\tLoss: 24.905347\n",
      "Train Epoch: 47 [20000/60000 (33%)]\tLoss: 17.410823\n",
      "Train Epoch: 47 [30000/60000 (50%)]\tLoss: 18.804614\n",
      "Train Epoch: 47 [40000/60000 (67%)]\tLoss: 25.990872\n",
      "Train Epoch: 47 [50000/60000 (83%)]\tLoss: 23.180378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 47 Average loss: 21.2045\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 23.626313\n",
      "Train Epoch: 48 [10000/60000 (17%)]\tLoss: 19.119679\n",
      "Train Epoch: 48 [20000/60000 (33%)]\tLoss: 20.106224\n",
      "Train Epoch: 48 [30000/60000 (50%)]\tLoss: 25.610911\n",
      "Train Epoch: 48 [40000/60000 (67%)]\tLoss: 22.562148\n",
      "Train Epoch: 48 [50000/60000 (83%)]\tLoss: 31.528975\n",
      "====> Epoch: 48 Average loss: 21.1820\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 26.014526\n",
      "Train Epoch: 49 [10000/60000 (17%)]\tLoss: 22.696699\n",
      "Train Epoch: 49 [20000/60000 (33%)]\tLoss: 20.632800\n",
      "Train Epoch: 49 [30000/60000 (50%)]\tLoss: 22.361338\n",
      "Train Epoch: 49 [40000/60000 (67%)]\tLoss: 25.069041\n",
      "Train Epoch: 49 [50000/60000 (83%)]\tLoss: 17.552471\n",
      "====> Epoch: 49 Average loss: 21.3136\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 24.942747\n",
      "Train Epoch: 50 [10000/60000 (17%)]\tLoss: 20.298806\n",
      "Train Epoch: 50 [20000/60000 (33%)]\tLoss: 26.034805\n",
      "Train Epoch: 50 [30000/60000 (50%)]\tLoss: 18.443668\n",
      "Train Epoch: 50 [40000/60000 (67%)]\tLoss: 21.796086\n",
      "Train Epoch: 50 [50000/60000 (83%)]\tLoss: 22.337578\n",
      "====> Epoch: 50 Average loss: 20.9593\n",
      "Train Epoch: 51 [0/60000 (0%)]\tLoss: 16.867799\n",
      "Train Epoch: 51 [10000/60000 (17%)]\tLoss: 25.675488\n",
      "Train Epoch: 51 [20000/60000 (33%)]\tLoss: 20.119532\n",
      "Train Epoch: 51 [30000/60000 (50%)]\tLoss: 19.948846\n",
      "Train Epoch: 51 [40000/60000 (67%)]\tLoss: 20.366858\n",
      "Train Epoch: 51 [50000/60000 (83%)]\tLoss: 19.537112\n",
      "====> Epoch: 51 Average loss: 21.2073\n",
      "Train Epoch: 52 [0/60000 (0%)]\tLoss: 25.510295\n",
      "Train Epoch: 52 [10000/60000 (17%)]\tLoss: 23.227227\n",
      "Train Epoch: 52 [20000/60000 (33%)]\tLoss: 21.843137\n",
      "Train Epoch: 52 [30000/60000 (50%)]\tLoss: 20.871257\n",
      "Train Epoch: 52 [40000/60000 (67%)]\tLoss: 23.232732\n",
      "Train Epoch: 52 [50000/60000 (83%)]\tLoss: 21.825615\n",
      "====> Epoch: 52 Average loss: 21.2231\n",
      "Train Epoch: 53 [0/60000 (0%)]\tLoss: 22.508936\n",
      "Train Epoch: 53 [10000/60000 (17%)]\tLoss: 15.893580\n",
      "Train Epoch: 53 [20000/60000 (33%)]\tLoss: 22.936658\n",
      "Train Epoch: 53 [30000/60000 (50%)]\tLoss: 21.718762\n",
      "Train Epoch: 53 [40000/60000 (67%)]\tLoss: 21.613955\n",
      "Train Epoch: 53 [50000/60000 (83%)]\tLoss: 14.607256\n",
      "====> Epoch: 53 Average loss: 21.0874\n",
      "Train Epoch: 54 [0/60000 (0%)]\tLoss: 16.886405\n",
      "Train Epoch: 54 [10000/60000 (17%)]\tLoss: 17.910280\n",
      "Train Epoch: 54 [20000/60000 (33%)]\tLoss: 23.084375\n",
      "Train Epoch: 54 [30000/60000 (50%)]\tLoss: 19.883754\n",
      "Train Epoch: 54 [40000/60000 (67%)]\tLoss: 20.505322\n",
      "Train Epoch: 54 [50000/60000 (83%)]\tLoss: 23.937659\n",
      "====> Epoch: 54 Average loss: 21.1265\n",
      "Train Epoch: 55 [0/60000 (0%)]\tLoss: 16.167416\n",
      "Train Epoch: 55 [10000/60000 (17%)]\tLoss: 20.598071\n",
      "Train Epoch: 55 [20000/60000 (33%)]\tLoss: 17.600360\n",
      "Train Epoch: 55 [30000/60000 (50%)]\tLoss: 20.657312\n",
      "Train Epoch: 55 [40000/60000 (67%)]\tLoss: 27.990989\n",
      "Train Epoch: 55 [50000/60000 (83%)]\tLoss: 21.660530\n",
      "====> Epoch: 55 Average loss: 21.3099\n",
      "Train Epoch: 56 [0/60000 (0%)]\tLoss: 20.220435\n",
      "Train Epoch: 56 [10000/60000 (17%)]\tLoss: 22.694563\n",
      "Train Epoch: 56 [20000/60000 (33%)]\tLoss: 23.433381\n",
      "Train Epoch: 56 [30000/60000 (50%)]\tLoss: 26.054324\n",
      "Train Epoch: 56 [40000/60000 (67%)]\tLoss: 23.969580\n",
      "Train Epoch: 56 [50000/60000 (83%)]\tLoss: 18.429283\n",
      "====> Epoch: 56 Average loss: 21.1703\n",
      "Train Epoch: 57 [0/60000 (0%)]\tLoss: 20.075540\n",
      "Train Epoch: 57 [10000/60000 (17%)]\tLoss: 19.434576\n",
      "Train Epoch: 57 [20000/60000 (33%)]\tLoss: 21.137466\n",
      "Train Epoch: 57 [30000/60000 (50%)]\tLoss: 25.993926\n",
      "Train Epoch: 57 [40000/60000 (67%)]\tLoss: 23.506638\n",
      "Train Epoch: 57 [50000/60000 (83%)]\tLoss: 21.114609\n",
      "====> Epoch: 57 Average loss: 21.0285\n",
      "Train Epoch: 58 [0/60000 (0%)]\tLoss: 17.791329\n",
      "Train Epoch: 58 [10000/60000 (17%)]\tLoss: 22.451101\n",
      "Train Epoch: 58 [20000/60000 (33%)]\tLoss: 26.377227\n",
      "Train Epoch: 58 [30000/60000 (50%)]\tLoss: 23.424490\n",
      "Train Epoch: 58 [40000/60000 (67%)]\tLoss: 24.045969\n",
      "Train Epoch: 58 [50000/60000 (83%)]\tLoss: 20.399504\n",
      "====> Epoch: 58 Average loss: 21.0322\n",
      "Train Epoch: 59 [0/60000 (0%)]\tLoss: 20.006537\n",
      "Train Epoch: 59 [10000/60000 (17%)]\tLoss: 18.706566\n",
      "Train Epoch: 59 [20000/60000 (33%)]\tLoss: 24.344673\n",
      "Train Epoch: 59 [30000/60000 (50%)]\tLoss: 25.043513\n",
      "Train Epoch: 59 [40000/60000 (67%)]\tLoss: 19.273225\n",
      "Train Epoch: 59 [50000/60000 (83%)]\tLoss: 18.290848\n",
      "====> Epoch: 59 Average loss: 21.1485\n",
      "Train Epoch: 60 [0/60000 (0%)]\tLoss: 16.770448\n",
      "Train Epoch: 60 [10000/60000 (17%)]\tLoss: 20.354653\n",
      "Train Epoch: 60 [20000/60000 (33%)]\tLoss: 18.677234\n",
      "Train Epoch: 60 [30000/60000 (50%)]\tLoss: 16.618696\n",
      "Train Epoch: 60 [40000/60000 (67%)]\tLoss: 23.039670\n",
      "Train Epoch: 60 [50000/60000 (83%)]\tLoss: 22.865574\n",
      "====> Epoch: 60 Average loss: 21.1512\n",
      "Train Epoch: 61 [0/60000 (0%)]\tLoss: 21.280156\n",
      "Train Epoch: 61 [10000/60000 (17%)]\tLoss: 22.527866\n",
      "Train Epoch: 61 [20000/60000 (33%)]\tLoss: 20.716956\n",
      "Train Epoch: 61 [30000/60000 (50%)]\tLoss: 21.928154\n",
      "Train Epoch: 61 [40000/60000 (67%)]\tLoss: 29.145063\n",
      "Train Epoch: 61 [50000/60000 (83%)]\tLoss: 24.460991\n",
      "Epoch    62: reducing learning rate of group 0 to 5.0000e-04.\n",
      "====> Epoch: 61 Average loss: 21.2624\n",
      "Train Epoch: 62 [0/60000 (0%)]\tLoss: 29.343340\n",
      "Train Epoch: 62 [10000/60000 (17%)]\tLoss: 22.620554\n",
      "Train Epoch: 62 [20000/60000 (33%)]\tLoss: 22.812361\n",
      "Train Epoch: 62 [30000/60000 (50%)]\tLoss: 19.879961\n",
      "Train Epoch: 62 [40000/60000 (67%)]\tLoss: 26.071948\n",
      "Train Epoch: 62 [50000/60000 (83%)]\tLoss: 18.229042\n",
      "====> Epoch: 62 Average loss: 20.9032\n",
      "Train Epoch: 63 [0/60000 (0%)]\tLoss: 18.676675\n",
      "Train Epoch: 63 [10000/60000 (17%)]\tLoss: 24.026418\n",
      "Train Epoch: 63 [20000/60000 (33%)]\tLoss: 17.525609\n",
      "Train Epoch: 63 [30000/60000 (50%)]\tLoss: 24.313423\n",
      "Train Epoch: 63 [40000/60000 (67%)]\tLoss: 22.117151\n",
      "Train Epoch: 63 [50000/60000 (83%)]\tLoss: 21.709771\n",
      "====> Epoch: 63 Average loss: 20.9617\n",
      "Train Epoch: 64 [0/60000 (0%)]\tLoss: 19.209844\n",
      "Train Epoch: 64 [10000/60000 (17%)]\tLoss: 20.664487\n",
      "Train Epoch: 64 [20000/60000 (33%)]\tLoss: 17.472875\n",
      "Train Epoch: 64 [30000/60000 (50%)]\tLoss: 17.723732\n",
      "Train Epoch: 64 [40000/60000 (67%)]\tLoss: 24.792502\n",
      "Train Epoch: 64 [50000/60000 (83%)]\tLoss: 17.133601\n",
      "====> Epoch: 64 Average loss: 21.0579\n",
      "Train Epoch: 65 [0/60000 (0%)]\tLoss: 23.331287\n",
      "Train Epoch: 65 [10000/60000 (17%)]\tLoss: 25.681582\n",
      "Train Epoch: 65 [20000/60000 (33%)]\tLoss: 22.029658\n",
      "Train Epoch: 65 [30000/60000 (50%)]\tLoss: 21.047969\n",
      "Train Epoch: 65 [40000/60000 (67%)]\tLoss: 16.889479\n",
      "Train Epoch: 65 [50000/60000 (83%)]\tLoss: 22.415164\n",
      "====> Epoch: 65 Average loss: 20.8959\n",
      "Train Epoch: 66 [0/60000 (0%)]\tLoss: 18.200693\n",
      "Train Epoch: 66 [10000/60000 (17%)]\tLoss: 20.154380\n",
      "Train Epoch: 66 [20000/60000 (33%)]\tLoss: 23.336802\n",
      "Train Epoch: 66 [30000/60000 (50%)]\tLoss: 22.432373\n",
      "Train Epoch: 66 [40000/60000 (67%)]\tLoss: 18.197219\n",
      "Train Epoch: 66 [50000/60000 (83%)]\tLoss: 16.625083\n",
      "====> Epoch: 66 Average loss: 21.0176\n",
      "Train Epoch: 67 [0/60000 (0%)]\tLoss: 22.289912\n",
      "Train Epoch: 67 [10000/60000 (17%)]\tLoss: 19.106310\n",
      "Train Epoch: 67 [20000/60000 (33%)]\tLoss: 27.015054\n",
      "Train Epoch: 67 [30000/60000 (50%)]\tLoss: 19.137373\n",
      "Train Epoch: 67 [40000/60000 (67%)]\tLoss: 17.203646\n",
      "Train Epoch: 67 [50000/60000 (83%)]\tLoss: 20.630208\n",
      "====> Epoch: 67 Average loss: 20.7911\n",
      "Train Epoch: 68 [0/60000 (0%)]\tLoss: 20.405703\n",
      "Train Epoch: 68 [10000/60000 (17%)]\tLoss: 24.248794\n",
      "Train Epoch: 68 [20000/60000 (33%)]\tLoss: 19.728723\n",
      "Train Epoch: 68 [30000/60000 (50%)]\tLoss: 18.630438\n",
      "Train Epoch: 68 [40000/60000 (67%)]\tLoss: 18.649745\n",
      "Train Epoch: 68 [50000/60000 (83%)]\tLoss: 23.963989\n",
      "====> Epoch: 68 Average loss: 20.6982\n",
      "Train Epoch: 69 [0/60000 (0%)]\tLoss: 20.889529\n",
      "Train Epoch: 69 [10000/60000 (17%)]\tLoss: 21.571863\n",
      "Train Epoch: 69 [20000/60000 (33%)]\tLoss: 20.442031\n",
      "Train Epoch: 69 [30000/60000 (50%)]\tLoss: 21.799563\n",
      "Train Epoch: 69 [40000/60000 (67%)]\tLoss: 19.248206\n",
      "Train Epoch: 69 [50000/60000 (83%)]\tLoss: 24.427195\n",
      "====> Epoch: 69 Average loss: 21.0411\n",
      "Train Epoch: 70 [0/60000 (0%)]\tLoss: 21.327520\n",
      "Train Epoch: 70 [10000/60000 (17%)]\tLoss: 19.561366\n",
      "Train Epoch: 70 [20000/60000 (33%)]\tLoss: 19.707740\n",
      "Train Epoch: 70 [30000/60000 (50%)]\tLoss: 20.505330\n",
      "Train Epoch: 70 [40000/60000 (67%)]\tLoss: 22.033420\n",
      "Train Epoch: 70 [50000/60000 (83%)]\tLoss: 21.888193\n",
      "====> Epoch: 70 Average loss: 20.9309\n",
      "Train Epoch: 71 [0/60000 (0%)]\tLoss: 20.092516\n",
      "Train Epoch: 71 [10000/60000 (17%)]\tLoss: 16.413331\n",
      "Train Epoch: 71 [20000/60000 (33%)]\tLoss: 19.702845\n",
      "Train Epoch: 71 [30000/60000 (50%)]\tLoss: 21.207104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 71 [40000/60000 (67%)]\tLoss: 14.991433\n",
      "Train Epoch: 71 [50000/60000 (83%)]\tLoss: 19.184192\n",
      "====> Epoch: 71 Average loss: 20.7182\n",
      "Train Epoch: 72 [0/60000 (0%)]\tLoss: 23.513962\n",
      "Train Epoch: 72 [10000/60000 (17%)]\tLoss: 23.862473\n",
      "Train Epoch: 72 [20000/60000 (33%)]\tLoss: 15.551178\n",
      "Train Epoch: 72 [30000/60000 (50%)]\tLoss: 18.873806\n",
      "Train Epoch: 72 [40000/60000 (67%)]\tLoss: 22.752783\n",
      "Train Epoch: 72 [50000/60000 (83%)]\tLoss: 20.580103\n",
      "====> Epoch: 72 Average loss: 20.9817\n",
      "Train Epoch: 73 [0/60000 (0%)]\tLoss: 24.547104\n",
      "Train Epoch: 73 [10000/60000 (17%)]\tLoss: 20.715947\n",
      "Train Epoch: 73 [20000/60000 (33%)]\tLoss: 17.923904\n",
      "Train Epoch: 73 [30000/60000 (50%)]\tLoss: 20.196490\n",
      "Train Epoch: 73 [40000/60000 (67%)]\tLoss: 18.700507\n",
      "Train Epoch: 73 [50000/60000 (83%)]\tLoss: 22.483079\n",
      "====> Epoch: 73 Average loss: 20.8917\n",
      "Train Epoch: 74 [0/60000 (0%)]\tLoss: 20.390251\n",
      "Train Epoch: 74 [10000/60000 (17%)]\tLoss: 21.169346\n",
      "Train Epoch: 74 [20000/60000 (33%)]\tLoss: 23.020046\n",
      "Train Epoch: 74 [30000/60000 (50%)]\tLoss: 20.378721\n",
      "Train Epoch: 74 [40000/60000 (67%)]\tLoss: 16.173558\n",
      "Train Epoch: 74 [50000/60000 (83%)]\tLoss: 20.619900\n",
      "====> Epoch: 74 Average loss: 20.7809\n",
      "Train Epoch: 75 [0/60000 (0%)]\tLoss: 18.865466\n",
      "Train Epoch: 75 [10000/60000 (17%)]\tLoss: 24.489619\n",
      "Train Epoch: 75 [20000/60000 (33%)]\tLoss: 23.989092\n",
      "Train Epoch: 75 [30000/60000 (50%)]\tLoss: 25.301772\n",
      "Train Epoch: 75 [40000/60000 (67%)]\tLoss: 18.848149\n",
      "Train Epoch: 75 [50000/60000 (83%)]\tLoss: 21.583943\n",
      "====> Epoch: 75 Average loss: 20.9465\n",
      "Train Epoch: 76 [0/60000 (0%)]\tLoss: 17.986597\n",
      "Train Epoch: 76 [10000/60000 (17%)]\tLoss: 22.340322\n",
      "Train Epoch: 76 [20000/60000 (33%)]\tLoss: 20.514219\n",
      "Train Epoch: 76 [30000/60000 (50%)]\tLoss: 22.018728\n",
      "Train Epoch: 76 [40000/60000 (67%)]\tLoss: 17.978463\n",
      "Train Epoch: 76 [50000/60000 (83%)]\tLoss: 22.581082\n",
      "====> Epoch: 76 Average loss: 20.7439\n",
      "Train Epoch: 77 [0/60000 (0%)]\tLoss: 23.034592\n",
      "Train Epoch: 77 [10000/60000 (17%)]\tLoss: 19.910994\n",
      "Train Epoch: 77 [20000/60000 (33%)]\tLoss: 20.627283\n",
      "Train Epoch: 77 [30000/60000 (50%)]\tLoss: 20.049789\n",
      "Train Epoch: 77 [40000/60000 (67%)]\tLoss: 16.097344\n",
      "Train Epoch: 77 [50000/60000 (83%)]\tLoss: 23.565491\n",
      "====> Epoch: 77 Average loss: 20.8823\n",
      "Train Epoch: 78 [0/60000 (0%)]\tLoss: 24.726257\n",
      "Train Epoch: 78 [10000/60000 (17%)]\tLoss: 20.250653\n",
      "Train Epoch: 78 [20000/60000 (33%)]\tLoss: 18.499956\n",
      "Train Epoch: 78 [30000/60000 (50%)]\tLoss: 19.587928\n",
      "Train Epoch: 78 [40000/60000 (67%)]\tLoss: 25.669150\n",
      "Train Epoch: 78 [50000/60000 (83%)]\tLoss: 18.997219\n",
      "====> Epoch: 78 Average loss: 21.0301\n",
      "Train Epoch: 79 [0/60000 (0%)]\tLoss: 16.923004\n",
      "Train Epoch: 79 [10000/60000 (17%)]\tLoss: 21.337769\n",
      "Train Epoch: 79 [20000/60000 (33%)]\tLoss: 25.232410\n",
      "Train Epoch: 79 [30000/60000 (50%)]\tLoss: 25.412246\n",
      "Train Epoch: 79 [40000/60000 (67%)]\tLoss: 17.293021\n",
      "Train Epoch: 79 [50000/60000 (83%)]\tLoss: 16.698082\n",
      "Epoch    80: reducing learning rate of group 0 to 2.5000e-04.\n",
      "====> Epoch: 79 Average loss: 20.9396\n",
      "Train Epoch: 80 [0/60000 (0%)]\tLoss: 27.357744\n",
      "Train Epoch: 80 [10000/60000 (17%)]\tLoss: 20.689868\n",
      "Train Epoch: 80 [20000/60000 (33%)]\tLoss: 20.735271\n",
      "Train Epoch: 80 [30000/60000 (50%)]\tLoss: 17.441909\n",
      "Train Epoch: 80 [40000/60000 (67%)]\tLoss: 20.714651\n",
      "Train Epoch: 80 [50000/60000 (83%)]\tLoss: 21.283506\n",
      "====> Epoch: 80 Average loss: 21.0493\n",
      "Train Epoch: 81 [0/60000 (0%)]\tLoss: 19.968329\n",
      "Train Epoch: 81 [10000/60000 (17%)]\tLoss: 20.217167\n",
      "Train Epoch: 81 [20000/60000 (33%)]\tLoss: 18.292899\n",
      "Train Epoch: 81 [30000/60000 (50%)]\tLoss: 19.005463\n",
      "Train Epoch: 81 [40000/60000 (67%)]\tLoss: 19.954512\n",
      "Train Epoch: 81 [50000/60000 (83%)]\tLoss: 21.976318\n",
      "====> Epoch: 81 Average loss: 20.6840\n",
      "Train Epoch: 82 [0/60000 (0%)]\tLoss: 18.654315\n",
      "Train Epoch: 82 [10000/60000 (17%)]\tLoss: 24.001565\n",
      "Train Epoch: 82 [20000/60000 (33%)]\tLoss: 21.308076\n",
      "Train Epoch: 82 [30000/60000 (50%)]\tLoss: 23.018247\n",
      "Train Epoch: 82 [40000/60000 (67%)]\tLoss: 21.728235\n",
      "Train Epoch: 82 [50000/60000 (83%)]\tLoss: 19.368867\n",
      "====> Epoch: 82 Average loss: 20.7288\n",
      "Train Epoch: 83 [0/60000 (0%)]\tLoss: 18.967733\n",
      "Train Epoch: 83 [10000/60000 (17%)]\tLoss: 19.842809\n",
      "Train Epoch: 83 [20000/60000 (33%)]\tLoss: 17.028635\n",
      "Train Epoch: 83 [30000/60000 (50%)]\tLoss: 19.802510\n",
      "Train Epoch: 83 [40000/60000 (67%)]\tLoss: 17.602019\n",
      "Train Epoch: 83 [50000/60000 (83%)]\tLoss: 19.899821\n",
      "====> Epoch: 83 Average loss: 20.9738\n",
      "Train Epoch: 84 [0/60000 (0%)]\tLoss: 17.063358\n",
      "Train Epoch: 84 [10000/60000 (17%)]\tLoss: 20.366992\n",
      "Train Epoch: 84 [20000/60000 (33%)]\tLoss: 21.719546\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0c6f786613b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-e7108cb55920>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;31m#data = sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-e7108cb55920>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m256.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-e7108cb55920>\u001b[0m in \u001b[0;36mlogit\u001b[0;34m(x, alpha)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1E-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "for i in range(150):\n",
    "    train(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
