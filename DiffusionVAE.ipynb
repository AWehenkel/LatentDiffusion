{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "def logit(x, alpha=1E-6):\n",
    "    y = alpha + (1.-2*alpha)*x\n",
    "    return np.log(y) - np.log(1. - y)\n",
    "\n",
    "def logit_back(x, alpha=1E-6):\n",
    "    y = torch.sigmoid(x)\n",
    "    return (y - alpha)/(1.-2*alpha)\n",
    "\n",
    "class AddUniformNoise(object):\n",
    "    def __init__(self, alpha=1E-6):\n",
    "        self.alpha = alpha\n",
    "    def __call__(self,samples):\n",
    "        samples = np.array(samples,dtype = np.float32)\n",
    "        samples += np.random.uniform(size = samples.shape)\n",
    "        samples = logit(samples/256., self.alpha)\n",
    "        return samples\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self,samples):\n",
    "        samples = torch.from_numpy(np.array(samples,dtype = np.float32)).float()\n",
    "        return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 100\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, download=True, transform=transforms.Compose([\n",
    "                       AddUniformNoise(),\n",
    "                       ToTensor()\n",
    "    #transforms.ToTensor()\n",
    "                   ]))\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       AddUniformNoise(),\n",
    "                       ToTensor()\n",
    "                       #transforms.ToTensor()\n",
    "                   ]))\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-6.5465, -6.5416, -6.5377, -6.5430, -6.5337, -6.5398, -6.5445, -6.5419,\n",
       "         -6.5440, -6.5391, -6.5383, -6.5458, -6.5421, -6.5434, -6.5438, -6.5419,\n",
       "         -6.5392, -6.5327, -6.5465, -6.5348, -6.5409, -6.5490, -6.5432, -6.5420,\n",
       "         -6.5393, -6.5467, -6.5407, -6.5384, -6.5403, -6.5375, -6.5447, -6.5398,\n",
       "         -6.5451, -6.5348, -6.5407, -6.5413, -6.5399, -6.5366, -6.5314, -6.5305,\n",
       "         -6.5351, -6.5325, -6.5323, -6.5302, -6.5382, -6.5338, -6.5429, -6.5368,\n",
       "         -6.5379, -6.5393, -6.5363, -6.5439, -6.5406, -6.5411, -6.5402, -6.5391,\n",
       "         -6.5400, -6.5335, -6.5355, -6.5420, -6.5443, -6.5364, -6.5362, -6.5363,\n",
       "         -6.5237, -6.5245, -6.4940, -6.4739, -6.4447, -6.4161, -6.3762, -6.3549,\n",
       "         -6.3553, -6.3675, -6.3938, -6.4338, -6.4740, -6.5084, -6.5285, -6.5276,\n",
       "         -6.5386, -6.5391, -6.5449, -6.5364, -6.5437, -6.5379, -6.5365, -6.5389,\n",
       "         -6.5391, -6.5375, -6.5210, -6.5069, -6.4756, -6.4216, -6.3536, -6.2548,\n",
       "         -6.1656, -6.0520, -5.9555, -5.8765, -5.8921, -5.9544, -6.0721, -6.1888,\n",
       "         -6.3272, -6.4138, -6.4857, -6.5241, -6.5375, -6.5423, -6.5465, -6.5401,\n",
       "         -6.5358, -6.5386, -6.5417, -6.5364, -6.5362, -6.5062, -6.4713, -6.3999,\n",
       "         -6.2808, -6.0977, -5.8567, -5.5532, -5.1867, -4.7991, -4.5001, -4.3259,\n",
       "         -4.3756, -4.6063, -4.9764, -5.3744, -5.7683, -6.0823, -6.2936, -6.4198,\n",
       "         -6.4879, -6.5222, -6.5436, -6.5428, -6.5330, -6.5403, -6.5364, -6.5412,\n",
       "         -6.5096, -6.4522, -6.3457, -6.1533, -5.8634, -5.4508, -4.9609, -4.3585,\n",
       "         -3.6942, -3.0238, -2.5141, -2.2648, -2.3412, -2.7211, -3.3549, -4.0909,\n",
       "         -4.8219, -5.4204, -5.8835, -6.1767, -6.3640, -6.4888, -6.5324, -6.5409,\n",
       "         -6.5446, -6.5384, -6.5429, -6.5223, -6.4762, -6.3586, -6.1508, -5.8044,\n",
       "         -5.3335, -4.7000, -3.9536, -3.1171, -2.3008, -1.5559, -1.0220, -0.7827,\n",
       "         -0.9010, -1.3132, -2.0258, -2.9432, -3.9177, -4.7905, -5.4703, -5.9450,\n",
       "         -6.2475, -6.4420, -6.5223, -6.5364, -6.5381, -6.5376, -6.5335, -6.5065,\n",
       "         -6.4059, -6.2208, -5.8937, -5.4102, -4.7302, -3.8939, -2.9592, -2.0360,\n",
       "         -1.2724, -0.6886, -0.3506, -0.2286, -0.3283, -0.6262, -1.2031, -2.0984,\n",
       "         -3.1873, -4.2442, -5.1280, -5.7662, -6.1682, -6.3969, -6.5084, -6.5387,\n",
       "         -6.5374, -6.5369, -6.5197, -6.4682, -6.3361, -6.0974, -5.6842, -5.0620,\n",
       "         -4.2369, -3.2328, -2.2346, -1.4344, -0.9693, -0.7853, -0.7806, -0.7880,\n",
       "         -0.7763, -0.8154, -1.0983, -1.8003, -2.8601, -3.9850, -4.9754, -5.7086,\n",
       "         -6.1605, -6.4008, -6.5115, -6.5346, -6.5436, -6.5344, -6.5185, -6.4577,\n",
       "         -6.3107, -6.0371, -5.5544, -4.8346, -3.8763, -2.7879, -1.8621, -1.3669,\n",
       "         -1.3411, -1.5777, -1.7808, -1.7686, -1.5520, -1.3068, -1.3259, -1.8667,\n",
       "         -2.8716, -4.0039, -4.9926, -5.7436, -6.2177, -6.4363, -6.5203, -6.5402,\n",
       "         -6.5463, -6.5364, -6.5046, -6.4520, -6.3142, -6.0288, -5.5010, -4.6982,\n",
       "         -3.6501, -2.5555, -1.8022, -1.6909, -2.0566, -2.4938, -2.6379, -2.4222,\n",
       "         -2.0159, -1.5821, -1.5305, -2.0835, -3.0989, -4.2008, -5.1237, -5.8424,\n",
       "         -6.2855, -6.4729, -6.5277, -6.5379, -6.5375, -6.5339, -6.5199, -6.4754,\n",
       "         -6.3400, -6.0343, -5.4709, -4.5681, -3.4535, -2.4030, -1.8864, -2.0411,\n",
       "         -2.5784, -2.9117, -2.8272, -2.4083, -1.8632, -1.4539, -1.5679, -2.3163,\n",
       "         -3.4037, -4.4580, -5.2854, -5.8916, -6.3217, -6.5012, -6.5326, -6.5351,\n",
       "         -6.5405, -6.5361, -6.5252, -6.4926, -6.3766, -6.0271, -5.3885, -4.4118,\n",
       "         -3.2456, -2.2845, -1.9259, -2.2165, -2.6636, -2.6693, -2.2784, -1.7077,\n",
       "         -1.1922, -1.0303, -1.4859, -2.4937, -3.6663, -4.6500, -5.3648, -5.9027,\n",
       "         -6.3073, -6.5183, -6.5287, -6.5386, -6.5398, -6.5331, -6.5257, -6.5054,\n",
       "         -6.3829, -5.9811, -5.2650, -4.2298, -3.0890, -2.2221, -1.9708, -2.1939,\n",
       "         -2.3424, -1.9188, -1.3262, -0.7315, -0.4652, -0.6515, -1.4267, -2.6366,\n",
       "         -3.8090, -4.6994, -5.3561, -5.8706, -6.2777, -6.5022, -6.5333, -6.5429,\n",
       "         -6.5390, -6.5459, -6.5392, -6.5117, -6.3748, -5.9016, -5.1219, -4.0987,\n",
       "         -3.0367, -2.2758, -2.0385, -2.0892, -1.8469, -1.1460, -0.4603, -0.0550,\n",
       "         -0.1458, -0.5976, -1.5001, -2.6938, -3.7896, -4.6260, -5.2835, -5.8217,\n",
       "         -6.2420, -6.4922, -6.5366, -6.5412, -6.5391, -6.5376, -6.5356, -6.5147,\n",
       "         -6.3400, -5.7961, -5.0162, -4.0652, -3.1352, -2.4734, -2.1892, -2.0322,\n",
       "         -1.5562, -0.8122, -0.2301, -0.0491, -0.3718, -0.8804, -1.7264, -2.7617,\n",
       "         -3.7152, -4.5356, -5.2274, -5.7998, -6.2253, -6.4816, -6.5325, -6.5396,\n",
       "         -6.5352, -6.5401, -6.5394, -6.5029, -6.3035, -5.6987, -4.9521, -4.1316,\n",
       "         -3.3647, -2.8213, -2.5324, -2.2579, -1.7749, -1.1397, -0.6493, -0.5550,\n",
       "         -0.8552, -1.3035, -1.9957, -2.8370, -3.6790, -4.5031, -5.2369, -5.8242,\n",
       "         -6.2427, -6.4781, -6.5332, -6.5369, -6.5348, -6.5309, -6.5306, -6.4797,\n",
       "         -6.2315, -5.6043, -4.9139, -4.2156, -3.6283, -3.2109, -2.9453, -2.6864,\n",
       "         -2.2916, -1.7287, -1.2418, -1.0836, -1.2322, -1.5715, -2.1306, -2.8636,\n",
       "         -3.6935, -4.5549, -5.3040, -5.8808, -6.2625, -6.4626, -6.5317, -6.5369,\n",
       "         -6.5431, -6.5382, -6.5255, -6.4560, -6.1510, -5.5204, -4.8521, -4.2229,\n",
       "         -3.7272, -3.3635, -3.1006, -2.8885, -2.5612, -2.0310, -1.4970, -1.2250,\n",
       "         -1.2678, -1.5698, -2.1268, -2.9058, -3.8237, -4.7023, -5.4180, -5.9552,\n",
       "         -6.2926, -6.4707, -6.5348, -6.5429, -6.5447, -6.5338, -6.5292, -6.4259,\n",
       "         -6.1003, -5.4754, -4.7698, -4.0992, -3.5628, -3.1298, -2.8295, -2.6070,\n",
       "         -2.2555, -1.7266, -1.2297, -0.9668, -1.0696, -1.4951, -2.1934, -3.1063,\n",
       "         -4.0862, -4.9403, -5.5987, -6.0685, -6.3432, -6.4801, -6.5308, -6.5346,\n",
       "         -6.5479, -6.5398, -6.5234, -6.4296, -6.1046, -5.5154, -4.7564, -3.9712,\n",
       "         -3.2772, -2.6948, -2.2581, -1.9057, -1.5041, -1.0279, -0.6712, -0.6345,\n",
       "         -0.9823, -1.6553, -2.5643, -3.5988, -4.5415, -5.2959, -5.8346, -6.1891,\n",
       "         -6.3963, -6.4971, -6.5328, -6.5402, -6.5404, -6.5304, -6.5279, -6.4365,\n",
       "         -6.1975, -5.6862, -4.9348, -4.0808, -3.2247, -2.4431, -1.7910, -1.2707,\n",
       "         -0.8000, -0.4586, -0.3729, -0.6892, -1.3521, -2.2867, -3.3284, -4.3070,\n",
       "         -5.1102, -5.6965, -6.0885, -6.3239, -6.4413, -6.5059, -6.5397, -6.5402,\n",
       "         -6.5401, -6.5440, -6.5273, -6.4835, -6.3233, -5.9554, -5.3432, -4.5496,\n",
       "         -3.6388, -2.7319, -1.9253, -1.3047, -0.8759, -0.7302, -0.9263, -1.5185,\n",
       "         -2.3724, -3.3604, -4.3053, -5.0941, -5.6667, -6.0569, -6.2857, -6.4220,\n",
       "         -6.4906, -6.5240, -6.5361, -6.5423, -6.5354, -6.5424, -6.5346, -6.5099,\n",
       "         -6.4378, -6.2554, -5.8795, -5.3048, -4.5499, -3.7060, -2.8748, -2.2415,\n",
       "         -1.9060, -1.9020, -2.2485, -2.9057, -3.7184, -4.5179, -5.2217, -5.7410,\n",
       "         -6.0800, -6.3022, -6.4137, -6.4851, -6.5242, -6.5343, -6.5403, -6.5444,\n",
       "         -6.5368, -6.5425, -6.5446, -6.5324, -6.5098, -6.4418, -6.2984, -6.0519,\n",
       "         -5.6805, -5.2012, -4.6965, -4.2950, -4.0828, -4.0921, -4.3316, -4.7161,\n",
       "         -5.1474, -5.5538, -5.9073, -6.1648, -6.3362, -6.4352, -6.4831, -6.5174,\n",
       "         -6.5351, -6.5400, -6.5473, -6.5358, -6.5343, -6.5351, -6.5393, -6.5421,\n",
       "         -6.5409, -6.5166, -6.4632, -6.3743, -6.2405, -6.0620, -5.8645, -5.7209,\n",
       "         -5.6575, -5.6750, -5.7595, -5.8843, -6.0155, -6.1500, -6.2787, -6.3806,\n",
       "         -6.4521, -6.4961, -6.5173, -6.5275, -6.5402, -6.5380, -6.5370, -6.5420,\n",
       "         -6.5454, -6.5469, -6.5455, -6.5399, -6.5362, -6.5267, -6.5019, -6.4907,\n",
       "         -6.4290, -6.3641, -6.2944, -6.2363, -6.2168, -6.2292, -6.2539, -6.3215,\n",
       "         -6.3610, -6.4155, -6.4543, -6.4849, -6.5159, -6.5314, -6.5406, -6.5436,\n",
       "         -6.5425, -6.5409, -6.5418, -6.5412, -6.5437, -6.5406, -6.5349, -6.5358,\n",
       "         -6.5359, -6.5339, -6.5374, -6.5315, -6.5405, -6.5428, -6.5238, -6.5217,\n",
       "         -6.5086, -6.5022, -6.5057, -6.5110, -6.5159, -6.5264, -6.5329, -6.5362,\n",
       "         -6.5359, -6.5324, -6.5484, -6.5447, -6.5404, -6.5364, -6.5337, -6.5408]),\n",
       " tensor([0.9960, 0.9896, 0.9850, 0.9907, 0.9807, 0.9929, 0.9887, 0.9910, 0.9964,\n",
       "         0.9934, 0.9908, 0.9958, 0.9944, 0.9906, 0.9984, 0.9960, 0.9935, 0.9858,\n",
       "         0.9927, 0.9888, 0.9869, 0.9988, 0.9941, 0.9872, 0.9906, 0.9985, 0.9980,\n",
       "         0.9920, 0.9894, 0.9960, 1.0054, 0.9890, 0.9975, 0.9871, 0.9956, 0.9952,\n",
       "         1.0101, 1.0049, 1.0146, 1.0268, 1.0306, 1.0278, 1.0320, 1.0286, 1.0337,\n",
       "         1.0249, 1.0193, 1.0059, 1.0043, 0.9969, 0.9904, 0.9993, 0.9937, 0.9950,\n",
       "         0.9883, 0.9899, 0.9893, 0.9925, 0.9909, 0.9913, 1.0049, 0.9939, 0.9999,\n",
       "         1.0220, 1.0444, 1.1057, 1.1597, 1.2452, 1.3267, 1.4317, 1.5058, 1.5801,\n",
       "         1.5938, 1.5468, 1.4536, 1.3592, 1.2276, 1.1211, 1.0573, 1.0028, 0.9963,\n",
       "         0.9894, 0.9974, 0.9943, 0.9975, 0.9904, 0.9855, 0.9898, 0.9878, 1.0066,\n",
       "         1.0273, 1.0905, 1.2111, 1.3810, 1.5918, 1.7962, 1.9980, 2.2114, 2.3979,\n",
       "         2.5138, 2.4890, 2.3818, 2.2047, 1.9500, 1.6482, 1.3929, 1.1940, 1.0853,\n",
       "         1.0329, 1.0006, 0.9934, 0.9893, 0.9802, 0.9932, 0.9957, 0.9971, 1.0099,\n",
       "         1.0704, 1.1860, 1.4018, 1.7145, 2.0919, 2.4979, 2.8925, 3.2869, 3.6451,\n",
       "         3.8905, 4.0115, 3.9441, 3.7789, 3.4793, 3.0899, 2.6112, 2.1101, 1.6889,\n",
       "         1.3651, 1.1573, 1.0256, 0.9988, 0.9891, 0.9862, 0.9872, 0.9949, 1.0140,\n",
       "         1.0678, 1.2625, 1.5669, 1.9688, 2.4602, 2.9842, 3.4788, 3.9173, 4.3010,\n",
       "         4.5982, 4.7511, 4.8067, 4.7518, 4.6532, 4.4172, 4.0806, 3.5741, 2.9852,\n",
       "         2.4101, 1.9078, 1.4661, 1.1481, 1.0215, 0.9912, 0.9981, 0.9877, 0.9995,\n",
       "         1.0341, 1.2045, 1.5270, 1.9916, 2.5349, 3.1389, 3.6978, 4.1618, 4.4797,\n",
       "         4.6945, 4.8020, 4.8484, 4.8428, 4.8242, 4.8243, 4.7680, 4.5818, 4.1835,\n",
       "         3.6016, 2.9527, 2.3164, 1.7578, 1.3077, 1.0585, 0.9881, 0.9885, 0.9919,\n",
       "         1.0116, 1.1194, 1.3822, 1.8315, 2.4131, 3.0415, 3.6462, 4.1511, 4.4772,\n",
       "         4.6304, 4.6786, 4.6723, 4.6831, 4.6631, 4.6522, 4.6796, 4.7351, 4.7173,\n",
       "         4.4857, 3.9828, 3.3158, 2.5867, 1.9139, 1.4018, 1.0820, 0.9989, 0.9900,\n",
       "         0.9979, 1.0583, 1.2321, 1.5890, 2.0837, 2.7193, 3.3810, 3.9839, 4.4233,\n",
       "         4.6471, 4.7163, 4.7131, 4.7221, 4.7288, 4.7026, 4.6883, 4.6947, 4.7469,\n",
       "         4.7661, 4.6073, 4.1646, 3.4758, 2.6682, 1.9341, 1.4013, 1.0807, 1.0005,\n",
       "         0.9982, 1.0042, 1.1019, 1.3146, 1.6799, 2.2128, 2.8835, 3.5909, 4.2019,\n",
       "         4.5924, 4.7592, 4.7631, 4.7540, 4.7256, 4.7154, 4.7054, 4.6854, 4.7176,\n",
       "         4.7649, 4.7818, 4.6290, 4.1757, 3.4631, 2.6004, 1.8231, 1.3025, 1.0628,\n",
       "         0.9996, 0.9993, 1.0088, 1.0977, 1.3006, 1.6693, 2.2263, 2.9443, 3.6869,\n",
       "         4.3095, 4.6682, 4.7835, 4.7644, 4.6616, 4.5255, 4.5256, 4.6041, 4.6358,\n",
       "         4.6937, 4.7459, 4.7458, 4.5469, 4.0556, 3.3386, 2.4787, 1.6804, 1.1935,\n",
       "         1.0258, 0.9929, 0.9866, 1.0076, 1.0765, 1.2401, 1.5758, 2.1857, 2.9848,\n",
       "         3.7762, 4.3999, 4.7166, 4.7788, 4.6981, 4.5257, 4.3904, 4.5307, 4.6573,\n",
       "         4.6413, 4.6923, 4.7489, 4.7159, 4.4033, 3.8663, 3.2007, 2.4208, 1.6043,\n",
       "         1.0946, 1.0133, 0.9783, 0.9907, 1.0019, 1.0443, 1.1646, 1.5065, 2.2015,\n",
       "         3.0679, 3.8995, 4.5018, 4.7627, 4.7831, 4.6943, 4.5488, 4.5193, 4.7685,\n",
       "         4.8079, 4.6666, 4.7217, 4.7782, 4.6515, 4.2536, 3.7310, 3.1397, 2.4531,\n",
       "         1.6413, 1.0801, 0.9955, 0.9938, 0.9915, 0.9917, 1.0130, 1.1176, 1.4905,\n",
       "         2.2937, 3.2095, 4.0353, 4.5724, 4.7823, 4.7923, 4.7156, 4.6371, 4.7134,\n",
       "         4.9468, 4.7770, 4.6237, 4.7511, 4.7938, 4.5891, 4.1898, 3.7044, 3.1609,\n",
       "         2.5304, 1.7469, 1.1028, 1.0053, 0.9978, 0.9909, 0.9966, 0.9998, 1.0938,\n",
       "         1.5162, 2.4435, 3.3783, 4.1390, 4.6158, 4.8112, 4.7873, 4.7162, 4.6761,\n",
       "         4.8088, 4.9362, 4.6610, 4.6177, 4.7886, 4.7811, 4.5892, 4.2395, 3.7931,\n",
       "         3.2473, 2.5988, 1.8162, 1.1465, 1.0170, 0.9913, 0.9940, 0.9917, 1.0026,\n",
       "         1.0951, 1.5718, 2.5977, 3.4861, 4.1683, 4.5804, 4.7713, 4.7622, 4.7147,\n",
       "         4.7143, 4.8500, 4.8534, 4.6516, 4.7216, 4.8130, 4.7387, 4.5904, 4.2943,\n",
       "         3.8634, 3.2961, 2.6012, 1.8204, 1.1901, 1.0212, 0.9923, 0.9919, 0.9963,\n",
       "         1.0061, 1.1085, 1.6855, 2.7278, 3.5532, 4.1220, 4.4713, 4.6488, 4.6641,\n",
       "         4.6724, 4.7602, 4.8709, 4.8419, 4.7600, 4.7864, 4.7651, 4.6865, 4.5876,\n",
       "         4.3272, 3.8807, 3.2785, 2.5402, 1.7905, 1.2240, 1.0269, 0.9888, 0.9891,\n",
       "         0.9854, 1.0109, 1.1610, 1.8518, 2.8528, 3.5938, 4.0638, 4.3509, 4.5005,\n",
       "         4.5613, 4.6255, 4.7335, 4.7975, 4.8259, 4.8068, 4.7529, 4.7109, 4.6884,\n",
       "         4.5916, 4.3301, 3.8364, 3.1675, 2.4425, 1.7383, 1.2379, 1.0329, 0.9890,\n",
       "         0.9941, 0.9910, 1.0193, 1.2658, 2.0281, 2.9730, 3.6507, 4.0708, 4.3140,\n",
       "         4.4592, 4.5605, 4.6199, 4.6587, 4.7467, 4.7972, 4.7603, 4.7061, 4.7172,\n",
       "         4.7357, 4.6039, 4.2753, 3.7312, 3.0411, 2.3200, 1.6725, 1.2271, 1.0235,\n",
       "         1.0029, 0.9999, 0.9849, 1.0465, 1.3547, 2.1484, 3.0574, 3.7375, 4.1546,\n",
       "         4.4049, 4.5710, 4.6790, 4.7176, 4.7283, 4.7779, 4.8045, 4.7616, 4.7015,\n",
       "         4.7701, 4.7379, 4.5367, 4.1339, 3.5284, 2.8448, 2.1429, 1.5647, 1.1761,\n",
       "         1.0150, 0.9911, 0.9934, 0.9991, 1.0529, 1.3871, 2.1114, 2.9929, 3.7270,\n",
       "         4.2402, 4.5473, 4.7000, 4.7791, 4.7867, 4.7909, 4.7846, 4.7636, 4.7486,\n",
       "         4.7393, 4.7634, 4.6520, 4.3621, 3.8348, 3.1765, 2.5020, 1.8927, 1.4419,\n",
       "         1.1287, 1.0123, 0.9948, 0.9875, 0.9829, 1.0515, 1.3106, 1.9149, 2.7325,\n",
       "         3.5353, 4.1723, 4.5772, 4.7757, 4.8201, 4.7759, 4.7069, 4.6664, 4.6802,\n",
       "         4.7309, 4.7341, 4.6667, 4.4222, 3.9496, 3.3369, 2.6780, 2.0909, 1.6283,\n",
       "         1.2911, 1.0658, 1.0084, 0.9945, 0.9920, 0.9975, 1.0230, 1.2076, 1.6155,\n",
       "         2.2770, 3.0688, 3.8078, 4.3704, 4.7097, 4.8562, 4.8688, 4.8053, 4.8043,\n",
       "         4.8298, 4.8232, 4.6812, 4.3970, 3.9213, 3.3348, 2.7121, 2.1463, 1.7013,\n",
       "         1.3800, 1.1500, 1.0287, 0.9921, 0.9905, 0.9898, 0.9901, 1.0074, 1.0735,\n",
       "         1.2993, 1.7354, 2.3622, 3.0838, 3.7556, 4.2859, 4.6110, 4.7642, 4.8087,\n",
       "         4.8005, 4.7366, 4.5546, 4.2219, 3.7363, 3.1766, 2.6063, 2.0882, 1.6712,\n",
       "         1.3680, 1.1849, 1.0537, 0.9978, 0.9937, 0.9953, 0.9892, 1.0000, 0.9981,\n",
       "         1.0163, 1.0962, 1.2811, 1.6317, 2.1357, 2.6999, 3.2591, 3.6977, 3.9845,\n",
       "         4.1009, 4.0866, 3.9344, 3.6296, 3.2636, 2.8342, 2.3923, 1.9717, 1.6137,\n",
       "         1.3459, 1.1663, 1.0735, 1.0144, 0.9936, 0.9929, 0.9873, 0.9882, 0.9861,\n",
       "         0.9915, 0.9953, 1.0286, 1.0816, 1.2504, 1.5052, 1.8553, 2.2086, 2.5186,\n",
       "         2.7042, 2.7947, 2.7921, 2.6537, 2.4493, 2.2482, 2.0043, 1.7533, 1.5054,\n",
       "         1.3000, 1.1548, 1.0601, 1.0153, 0.9992, 0.9828, 0.9834, 0.9897, 0.9958,\n",
       "         0.9880, 0.9994, 0.9825, 0.9973, 1.0207, 1.0883, 1.2181, 1.3758, 1.5453,\n",
       "         1.6964, 1.8199, 1.8626, 1.8539, 1.7734, 1.6561, 1.5478, 1.4233, 1.2859,\n",
       "         1.1681, 1.1046, 1.0428, 1.0082, 1.0060, 0.9926, 0.9905, 0.9908, 0.9959,\n",
       "         0.9935, 0.9854, 0.9797, 0.9842, 0.9900, 0.9893, 0.9973, 1.0079, 1.0244,\n",
       "         1.0424, 1.0501, 1.0783, 1.0924, 1.1000, 1.1192, 1.0989, 1.0828, 1.0638,\n",
       "         1.0318, 1.0090, 0.9953, 0.9903, 1.0086, 0.9988, 0.9931, 0.9876, 0.9721,\n",
       "         0.9946]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Mean abd std per pixel\n",
    "x_mean = 0\n",
    "x_mean2 = 0\n",
    "for batch_idx, (cur_x, target) in enumerate(train_loader):\n",
    "    cur_x = cur_x.view(bs, -1).float()\n",
    "    x_mean += cur_x.mean(0)\n",
    "    x_mean2 += (cur_x ** 2).mean(0)\n",
    "x_mean /= batch_idx + 1\n",
    "x_std = (x_mean2 / (batch_idx + 1) - x_mean ** 2) ** .5\n",
    "x_mean, x_std\n",
    "x_std[x_std == 0.] = 1.\n",
    "x_mean, x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataDiffuser(nn.Module):\n",
    "    def __init__(self, beta_min=1e-4, beta_max=.02, t_min=1, t_max=1000):\n",
    "        super(dataDiffuser, self).__init__()\n",
    "        self.register_buffer('betas', torch.arange(beta_min, beta_max + 1e-10, (beta_max - beta_min) / (t_max - t_min)))\n",
    "        self.register_buffer('alphas_t', (1 - self.betas))\n",
    "        self.register_buffer('alphas', self.alphas_t.log().cumsum(0).exp())\n",
    "\n",
    "    def diffuse(self, x_t0, t, t0=0):\n",
    "        \n",
    "        alpha_t0 = 1 * (t0 == 0).float() + (1 - (t0 == 0).float()) * self.alphas[t0-1]\n",
    "\n",
    "        mu = x_t0*(self.alphas[t]/alpha_t0).sqrt().unsqueeze(1).expand(-1, x_t0.shape[1]).float()\n",
    "        #mu = x_t0 * self.alphas[t].sqrt().unsqueeze(1).expand(-1, x_t0.shape[1]).float()\n",
    "        sigma_t = ((self.alphas[t]/alpha_t0) * (1 - alpha_t0) + (1 - self.alphas[t])).sqrt()\n",
    "        sigma = sigma_t.unsqueeze(1).expand(-1, x_t0.shape[1]).float()\n",
    "        #sigma = (1 - self.alphas[t].unsqueeze(1).expand(-1, x_t0.shape[1]).float()).sqrt()\n",
    "        return mu + torch.randn(x_t0.shape).to(x_t0.device) * sigma, sigma_t\n",
    "    \n",
    "    def prevMean(self, x_t, x_0, t):\n",
    "        alphas = self.alphas.unsqueeze(1).expand(-1, x_t.shape[1]).float()\n",
    "        betas = self.betas.unsqueeze(1).expand(-1, x_t.shape[1]).float()\n",
    "        alphas_t = self.alphas_t.unsqueeze(1).expand(-1,x_t.shape[1]).float()\n",
    "        mu = alphas[t - 1].sqrt() * betas[t] * x_0/(1 - alphas[t]) + alphas_t[t].sqrt()*(1 - alphas[t-1])*x_t/(1 - alphas[t])\n",
    "        sigma = ((1 - self.alphas[t-1])/(1 - self.alphas[t]) * self.betas[t]).sqrt()\n",
    "        return mu, sigma\n",
    "\n",
    "class TemporalDecoder(nn.Module):\n",
    "    def __init__(self, x_dim, z_dim, h_dim, t_dim=1):\n",
    "        super(TemporalDecoder, self).__init__()\n",
    "        # decoder part\n",
    "        self.net = nn.Sequential(nn.Linear(z_dim + t_dim, h_dim), nn.ReLU(),\n",
    "                                 nn.Linear(h_dim, h_dim), nn.ReLU(),\n",
    "                                 nn.Linear(h_dim, h_dim), nn.ReLU(),                                 \n",
    "                                 #nn.Linear(h_dim, h_dim), nn.ReLU(),                               \n",
    "                                 #nn.Linear(h_dim, h_dim), nn.ReLU(),\n",
    "                                 nn.Linear(h_dim, x_dim))\n",
    "        \n",
    "    def forward(self, z, t):\n",
    "        return self.net(torch.cat((z, t), 1))\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, t):\n",
    "        emb = t/torch.exp(torch.arange(self.dim).float()/self.dim * torch.log(torch.ones(1, self.dim) * 100)).to(t.device)\n",
    "        return torch.cat((torch.sin(emb), torch.cos(emb)), 1)\n",
    "\n",
    "class StupidPositionalEncoder(nn.Module):\n",
    "    def __init__(self, T_MAX):\n",
    "        super(StupidPositionalEncoder, self).__init__()\n",
    "        self.T_MAX = T_MAX\n",
    "        \n",
    "    def forward(self, t):\n",
    "        return t.float()/self.T_MAX\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x_dim, z_dim, h_dim):\n",
    "            super(Encoder, self).__init__()\n",
    "            # decoder part\n",
    "            self.net = nn.Sequential(nn.Linear(x_dim, h_dim), nn.ReLU(),\n",
    "                                     nn.Linear(h_dim, h_dim), nn.ReLU(),                                     \n",
    "                                     nn.Linear(h_dim, h_dim), nn.ReLU(),                                  \n",
    "                                     nn.Linear(h_dim, h_dim), nn.ReLU(),                               \n",
    "                                     nn.Linear(h_dim, h_dim), nn.ReLU(),\n",
    "                                     nn.Linear(h_dim, z_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class TemporalEncoder(nn.Module):\n",
    "    def __init__(self, x_dim, z_dim, h_dim, t_dim=1):\n",
    "            super(TemporalEncoder, self).__init__()\n",
    "            # decoder part\n",
    "            self.net = nn.Sequential(nn.Linear(x_dim + t_dim, h_dim), nn.ReLU(),\n",
    "                                     nn.Linear(h_dim, h_dim), nn.ReLU(),                                     \n",
    "                                     nn.Linear(h_dim, h_dim), nn.ReLU(),                                  \n",
    "                                     #nn.Linear(h_dim, h_dim), nn.ReLU(),                               \n",
    "                                     #nn.Linear(h_dim, h_dim), nn.ReLU(),\n",
    "                                     nn.Linear(h_dim, z_dim))\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        return self.net(torch.cat((x, t), 1))\n",
    "    \n",
    "class TransitionNet(nn.Module):\n",
    "    def __init__(self, z_dim, h_dim, t_dim=1):\n",
    "        super(TransitionNet, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(z_dim + t_dim, h_dim), nn.ReLU(),\n",
    "                                 nn.Linear(h_dim, h_dim), nn.ReLU(),\n",
    "                                 nn.Linear(h_dim, h_dim), nn.ReLU(),                                  \n",
    "                                 #nn.Linear(h_dim, h_dim), nn.ReLU(),                               \n",
    "                                 #nn.Linear(h_dim, h_dim), nn.ReLU(),\n",
    "                                 nn.Linear(h_dim, z_dim))\n",
    "    def forward(self, z, t):\n",
    "        return self.net(torch.cat((z, t), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1000, 0.2464, 0.3842, 0.5115, 0.6252, 0.7229, 0.8034, 0.8668, 0.9143,\n",
       "         0.9479, 0.9702, 0.9842, 0.9922, 0.9965, 0.9985, 0.9995, 0.9998, 0.9999,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        device='cuda:0'),\n",
       " tensor([9.9499e-01, 9.6916e-01, 9.2324e-01, 8.5929e-01, 7.8049e-01, 6.9096e-01,\n",
       "         5.9539e-01, 4.9858e-01, 4.0505e-01, 3.1862e-01, 2.4213e-01, 1.7731e-01,\n",
       "         1.2475e-01, 8.4031e-02, 5.3970e-02, 3.2884e-02, 1.8890e-02, 1.0151e-02,\n",
       "         5.0500e-03, 2.2934e-03, 9.3160e-04, 3.2772e-04, 9.4130e-05, 1.9118e-05,\n",
       "         0.0000e+00], device='cuda:0'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_MAX = 25\n",
    "latent_s = 25\n",
    "t_emb_s = 1\n",
    "pos_enc = StupidPositionalEncoder(T_MAX)#PositionalEncoder(t_emb_s//2)#\n",
    "dev = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "dec = TemporalDecoder(784, latent_s, 256, t_emb_s).to(dev)\n",
    "enc = TemporalEncoder(784, latent_s, 256, t_emb_s).to(dev)\n",
    "trans = TransitionNet(latent_s, 100, t_emb_s).to(dev)\n",
    "dif = dataDiffuser(beta_min=1e-2, beta_max=1., t_max=T_MAX).to(dev)\n",
    "sampling_t0 = False\n",
    "(1 - dif.alphas).sqrt(), (dif.alphas).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(dec.parameters()) + list(enc.parameters()) + list(trans.parameters()), lr=.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, threshold=0.001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_X_back(x):\n",
    "    nb_x = x.shape[0]\n",
    "    x = x * x_std.to(dev).unsqueeze(0).expand(nb_x, -1) + x_mean.to(dev).unsqueeze(0).expand(nb_x, -1)\n",
    "    return logit_back(x)\n",
    "\n",
    "def train(epoch):\n",
    "    \n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        x0 = data.view(data.shape[0], -1).to(dev)\n",
    "        \n",
    "        x0 = (x0 - x_mean.to(dev).unsqueeze(0).expand(bs, -1))/x_std.to(dev).unsqueeze(0).expand(bs, -1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if sampling_t0:\n",
    "            t0 = torch.randint(0, T_MAX - 1, [x0.shape[0]]).to(dev)\n",
    "            x_t0, sigma_x_t0 = dif.diffuse(x0, t0, torch.zeros(x0.shape[0]).long().to(dev))\n",
    "        else:\n",
    "            t0 = torch.zeros(x0.shape[0]).to(dev).long()\n",
    "            x_t0 = x0\n",
    "        \n",
    "        \n",
    "        z_t0 = enc(x_t0, pos_enc(t0.float().unsqueeze(1)))\n",
    "        #z_t0 = z_t0 + torch.randn(z_t0.shape).to(dev) * (1 - dif.alphas[t0]).sqrt().unsqueeze(1).expand(-1, z_t0.shape[1])\n",
    "        t = torch.torch.distributions.Uniform(t0.float() + 1, torch.ones_like(t0) * T_MAX).sample().long().to(dev)\n",
    "                \n",
    "        z_t, sigma_z = dif.diffuse(z_t0, t, t0)\n",
    "        x_t, sigma_x = dif.diffuse(x_t0, t, t0)\n",
    "        \n",
    "        \n",
    "        mu_x_pred = dec(z_t, pos_enc(t.float().unsqueeze(1)))\n",
    "        KL_x = ((mu_x_pred - x_t)**2).sum(1) / sigma_x**2\n",
    "        \n",
    "        mu_z_pred = trans(z_t, pos_enc(t.float().unsqueeze(1)))\n",
    "        mu, sigma = dif.prevMean(z_t0, z_t, t)\n",
    "        KL_z = ((mu - mu_z_pred)**2).sum(1) / sigma**2\n",
    "        \n",
    "        loss = KL_x.mean(0) + KL_z.mean(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    scheduler.step(train_loss)\n",
    "    zT = torch.randn(64, latent_s).to(dev)\n",
    "    z_t = zT\n",
    "    for t in range(T_MAX - 1, 0, -1):\n",
    "        t_t = torch.ones(64, 1).to(dev) * t\n",
    "        if t > 0:\n",
    "            sigma = ((1 - dif.alphas[t-1])/(1 - dif.alphas[t]) * dif.betas[t]).sqrt()\n",
    "        else:\n",
    "            sigma = 0\n",
    "        z_t = trans(z_t, pos_enc(t_t))  + torch.randn(z_t.shape).to(dev) * sigma \n",
    "        if (t - 1) % 1 == 0:\n",
    "            x_t = dec(z_t, pos_enc(t_t - 1))\n",
    "            save_image(get_X_back(x_t).view(64, 1, 28, 28), './Samples/Generated/sample_gen_' + str(epoch) + '_' + str(t - 1) + '.png')\n",
    "            x_t, _ = dif.diffuse(x0, (torch.ones(x0.shape[0]).to(dev) * t - 1).long(), torch.zeros(x0.shape[0]).long().to(dev))\n",
    "            save_image(get_X_back(x_t).view(x0.shape[0], 1, 28, 28), './Samples/Real/sample_real_' + str(epoch) + '_' + str(t - 1) + '.png')\n",
    "                \n",
    "    \n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 13.892440\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 13.091863\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 12.390828\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 13.525935\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 12.066187\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 12.410906\n",
      "====> Epoch: 0 Average loss: 13.6109\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 14.230599\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 13.790848\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 13.927562\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 11.545673\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 14.576415\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 13.358224\n",
      "====> Epoch: 1 Average loss: 13.6508\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 12.034630\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 12.477793\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 12.906927\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 12.213268\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 12.629274\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 14.402852\n",
      "====> Epoch: 2 Average loss: 13.6467\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 14.711984\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 11.004713\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 11.892777\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 14.522366\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 13.674017\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 12.760490\n",
      "====> Epoch: 3 Average loss: 13.5321\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 13.800139\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 13.247739\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 12.146505\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 12.672966\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 13.466719\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 13.349619\n",
      "====> Epoch: 4 Average loss: 13.5222\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 14.371841\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 14.646726\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 14.335835\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 16.643129\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 13.878544\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 14.815881\n",
      "====> Epoch: 5 Average loss: 13.5984\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 12.582413\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 13.032472\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 13.110248\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 13.754866\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 12.688252\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 14.711285\n",
      "====> Epoch: 6 Average loss: 13.5132\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 15.440790\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 11.589225\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 11.745172\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 13.025037\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 11.066412\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 14.170139\n",
      "====> Epoch: 7 Average loss: 13.6559\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 16.717634\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 14.185681\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 12.938948\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 17.146561\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 15.418574\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 13.140441\n",
      "Epoch   156: reducing learning rate of group 0 to 3.9063e-06.\n",
      "====> Epoch: 8 Average loss: 13.5622\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 15.153542\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 12.295269\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 13.837852\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 13.012649\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 14.480536\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 12.201764\n",
      "====> Epoch: 9 Average loss: 13.5851\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 11.180593\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 13.865046\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 14.499825\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 11.551023\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 14.330532\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 16.308417\n",
      "====> Epoch: 10 Average loss: 13.6523\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 10.105442\n",
      "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 12.672688\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 13.579264\n",
      "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 11.064846\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 14.074021\n",
      "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 13.246029\n",
      "====> Epoch: 11 Average loss: 13.5469\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 13.838840\n",
      "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 15.127484\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 14.257782\n",
      "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 13.646079\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 13.733597\n",
      "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 14.544751\n",
      "====> Epoch: 12 Average loss: 13.6851\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 14.061357\n",
      "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 15.355212\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 16.521211\n",
      "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 15.278390\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 13.138926\n",
      "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 14.566677\n",
      "====> Epoch: 13 Average loss: 13.7257\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 14.390942\n",
      "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 15.063915\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 12.388447\n",
      "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 13.648932\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 14.014512\n",
      "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 12.516500\n",
      "====> Epoch: 14 Average loss: 13.5980\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 11.557753\n",
      "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 14.376714\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 12.899712\n",
      "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 12.953793\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 13.095183\n",
      "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 13.167179\n",
      "====> Epoch: 15 Average loss: 13.5527\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 12.487959\n",
      "Train Epoch: 16 [10000/60000 (17%)]\tLoss: 12.705798\n",
      "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 15.758944\n",
      "Train Epoch: 16 [30000/60000 (50%)]\tLoss: 15.886750\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 12.541906\n",
      "Train Epoch: 16 [50000/60000 (83%)]\tLoss: 13.496444\n",
      "====> Epoch: 16 Average loss: 13.5808\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 14.394489\n",
      "Train Epoch: 17 [10000/60000 (17%)]\tLoss: 13.541714\n",
      "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 15.030688\n",
      "Train Epoch: 17 [30000/60000 (50%)]\tLoss: 12.424763\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 12.990409\n",
      "Train Epoch: 17 [50000/60000 (83%)]\tLoss: 15.295139\n",
      "====> Epoch: 17 Average loss: 13.6408\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 13.130240\n",
      "Train Epoch: 18 [10000/60000 (17%)]\tLoss: 15.182915\n",
      "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 14.556382\n",
      "Train Epoch: 18 [30000/60000 (50%)]\tLoss: 13.194576\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 14.191663\n",
      "Train Epoch: 18 [50000/60000 (83%)]\tLoss: 15.437467\n",
      "====> Epoch: 18 Average loss: 13.7192\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 13.779934\n",
      "Train Epoch: 19 [10000/60000 (17%)]\tLoss: 14.792848\n",
      "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 12.362847\n",
      "Train Epoch: 19 [30000/60000 (50%)]\tLoss: 12.142045\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 12.253531\n",
      "Train Epoch: 19 [50000/60000 (83%)]\tLoss: 10.764906\n",
      "====> Epoch: 19 Average loss: 13.4530\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 13.188610\n",
      "Train Epoch: 20 [10000/60000 (17%)]\tLoss: 11.996072\n",
      "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 14.080791\n",
      "Train Epoch: 20 [30000/60000 (50%)]\tLoss: 16.926305\n",
      "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 17.452596\n",
      "Train Epoch: 20 [50000/60000 (83%)]\tLoss: 11.755393\n",
      "====> Epoch: 20 Average loss: 13.5762\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 14.604290\n",
      "Train Epoch: 21 [10000/60000 (17%)]\tLoss: 13.726151\n",
      "Train Epoch: 21 [20000/60000 (33%)]\tLoss: 12.552434\n",
      "Train Epoch: 21 [30000/60000 (50%)]\tLoss: 13.849979\n",
      "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 15.668738\n",
      "Train Epoch: 21 [50000/60000 (83%)]\tLoss: 15.867433\n",
      "====> Epoch: 21 Average loss: 13.5473\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 15.811547\n",
      "Train Epoch: 22 [10000/60000 (17%)]\tLoss: 13.725934\n",
      "Train Epoch: 22 [20000/60000 (33%)]\tLoss: 12.003383\n",
      "Train Epoch: 22 [30000/60000 (50%)]\tLoss: 12.478721\n",
      "Train Epoch: 22 [40000/60000 (67%)]\tLoss: 14.529528\n",
      "Train Epoch: 22 [50000/60000 (83%)]\tLoss: 11.402092\n",
      "====> Epoch: 22 Average loss: 13.5870\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 20.266268\n",
      "Train Epoch: 23 [10000/60000 (17%)]\tLoss: 11.600139\n",
      "Train Epoch: 23 [20000/60000 (33%)]\tLoss: 14.205671\n",
      "Train Epoch: 23 [30000/60000 (50%)]\tLoss: 14.818010\n",
      "Train Epoch: 23 [40000/60000 (67%)]\tLoss: 12.995496\n",
      "Train Epoch: 23 [50000/60000 (83%)]\tLoss: 12.456626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 23 Average loss: 13.6704\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 12.593188\n",
      "Train Epoch: 24 [10000/60000 (17%)]\tLoss: 15.936273\n",
      "Train Epoch: 24 [20000/60000 (33%)]\tLoss: 12.901859\n",
      "Train Epoch: 24 [30000/60000 (50%)]\tLoss: 12.651404\n",
      "Train Epoch: 24 [40000/60000 (67%)]\tLoss: 15.275785\n",
      "Train Epoch: 24 [50000/60000 (83%)]\tLoss: 13.026960\n",
      "====> Epoch: 24 Average loss: 13.6430\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 14.627281\n",
      "Train Epoch: 25 [10000/60000 (17%)]\tLoss: 14.097472\n",
      "Train Epoch: 25 [20000/60000 (33%)]\tLoss: 13.885525\n",
      "Train Epoch: 25 [30000/60000 (50%)]\tLoss: 13.300404\n",
      "Train Epoch: 25 [40000/60000 (67%)]\tLoss: 12.762391\n",
      "Train Epoch: 25 [50000/60000 (83%)]\tLoss: 12.522054\n",
      "====> Epoch: 25 Average loss: 13.6136\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 15.002812\n",
      "Train Epoch: 26 [10000/60000 (17%)]\tLoss: 15.264357\n",
      "Train Epoch: 26 [20000/60000 (33%)]\tLoss: 12.202352\n",
      "Train Epoch: 26 [30000/60000 (50%)]\tLoss: 12.617859\n",
      "Train Epoch: 26 [40000/60000 (67%)]\tLoss: 12.538772\n",
      "Train Epoch: 26 [50000/60000 (83%)]\tLoss: 15.555281\n",
      "====> Epoch: 26 Average loss: 13.5350\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 11.778960\n",
      "Train Epoch: 27 [10000/60000 (17%)]\tLoss: 13.549498\n",
      "Train Epoch: 27 [20000/60000 (33%)]\tLoss: 13.700162\n",
      "Train Epoch: 27 [30000/60000 (50%)]\tLoss: 13.568424\n",
      "Train Epoch: 27 [40000/60000 (67%)]\tLoss: 11.892554\n",
      "Train Epoch: 27 [50000/60000 (83%)]\tLoss: 12.237184\n",
      "====> Epoch: 27 Average loss: 13.5284\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 14.043440\n",
      "Train Epoch: 28 [10000/60000 (17%)]\tLoss: 13.730612\n",
      "Train Epoch: 28 [20000/60000 (33%)]\tLoss: 13.809749\n",
      "Train Epoch: 28 [30000/60000 (50%)]\tLoss: 12.303556\n",
      "Train Epoch: 28 [40000/60000 (67%)]\tLoss: 13.045747\n",
      "Train Epoch: 28 [50000/60000 (83%)]\tLoss: 14.243221\n",
      "====> Epoch: 28 Average loss: 13.5425\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 12.376567\n",
      "Train Epoch: 29 [10000/60000 (17%)]\tLoss: 15.945369\n",
      "Train Epoch: 29 [20000/60000 (33%)]\tLoss: 12.921663\n",
      "Train Epoch: 29 [30000/60000 (50%)]\tLoss: 20.024387\n",
      "Train Epoch: 29 [40000/60000 (67%)]\tLoss: 13.297728\n",
      "Train Epoch: 29 [50000/60000 (83%)]\tLoss: 16.080146\n",
      "====> Epoch: 29 Average loss: 13.6714\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 14.601860\n",
      "Train Epoch: 30 [10000/60000 (17%)]\tLoss: 15.089530\n",
      "Train Epoch: 30 [20000/60000 (33%)]\tLoss: 12.565144\n",
      "Train Epoch: 30 [30000/60000 (50%)]\tLoss: 13.357904\n",
      "Train Epoch: 30 [40000/60000 (67%)]\tLoss: 16.367334\n",
      "Train Epoch: 30 [50000/60000 (83%)]\tLoss: 13.517620\n",
      "Epoch   178: reducing learning rate of group 0 to 1.9531e-06.\n",
      "====> Epoch: 30 Average loss: 13.5216\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 12.444421\n",
      "Train Epoch: 31 [10000/60000 (17%)]\tLoss: 16.414177\n",
      "Train Epoch: 31 [20000/60000 (33%)]\tLoss: 13.997583\n",
      "Train Epoch: 31 [30000/60000 (50%)]\tLoss: 13.415125\n",
      "Train Epoch: 31 [40000/60000 (67%)]\tLoss: 14.349872\n",
      "Train Epoch: 31 [50000/60000 (83%)]\tLoss: 13.941792\n",
      "====> Epoch: 31 Average loss: 13.6131\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 14.219729\n",
      "Train Epoch: 32 [10000/60000 (17%)]\tLoss: 13.584292\n",
      "Train Epoch: 32 [20000/60000 (33%)]\tLoss: 9.888091\n",
      "Train Epoch: 32 [30000/60000 (50%)]\tLoss: 13.645209\n",
      "Train Epoch: 32 [40000/60000 (67%)]\tLoss: 12.016714\n",
      "Train Epoch: 32 [50000/60000 (83%)]\tLoss: 13.170817\n",
      "====> Epoch: 32 Average loss: 13.5899\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 13.327739\n",
      "Train Epoch: 33 [10000/60000 (17%)]\tLoss: 12.566981\n",
      "Train Epoch: 33 [20000/60000 (33%)]\tLoss: 14.976066\n",
      "Train Epoch: 33 [30000/60000 (50%)]\tLoss: 14.607012\n",
      "Train Epoch: 33 [40000/60000 (67%)]\tLoss: 12.333618\n",
      "Train Epoch: 33 [50000/60000 (83%)]\tLoss: 12.570222\n",
      "====> Epoch: 33 Average loss: 13.5992\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 13.861796\n",
      "Train Epoch: 34 [10000/60000 (17%)]\tLoss: 12.344026\n",
      "Train Epoch: 34 [20000/60000 (33%)]\tLoss: 13.656383\n",
      "Train Epoch: 34 [30000/60000 (50%)]\tLoss: 12.271146\n",
      "Train Epoch: 34 [40000/60000 (67%)]\tLoss: 13.746809\n",
      "Train Epoch: 34 [50000/60000 (83%)]\tLoss: 16.045546\n",
      "====> Epoch: 34 Average loss: 13.5514\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 9.230646\n",
      "Train Epoch: 35 [10000/60000 (17%)]\tLoss: 15.184528\n",
      "Train Epoch: 35 [20000/60000 (33%)]\tLoss: 13.827271\n",
      "Train Epoch: 35 [30000/60000 (50%)]\tLoss: 13.316140\n",
      "Train Epoch: 35 [40000/60000 (67%)]\tLoss: 12.588420\n",
      "Train Epoch: 35 [50000/60000 (83%)]\tLoss: 15.529939\n",
      "====> Epoch: 35 Average loss: 13.5727\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 13.885460\n",
      "Train Epoch: 36 [10000/60000 (17%)]\tLoss: 14.835382\n",
      "Train Epoch: 36 [20000/60000 (33%)]\tLoss: 11.925793\n",
      "Train Epoch: 36 [30000/60000 (50%)]\tLoss: 10.844025\n",
      "Train Epoch: 36 [40000/60000 (67%)]\tLoss: 13.458640\n",
      "Train Epoch: 36 [50000/60000 (83%)]\tLoss: 13.664603\n",
      "====> Epoch: 36 Average loss: 13.6403\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 14.157400\n",
      "Train Epoch: 37 [10000/60000 (17%)]\tLoss: 11.379253\n",
      "Train Epoch: 37 [20000/60000 (33%)]\tLoss: 13.866927\n",
      "Train Epoch: 37 [30000/60000 (50%)]\tLoss: 10.847762\n",
      "Train Epoch: 37 [40000/60000 (67%)]\tLoss: 15.654893\n",
      "Train Epoch: 37 [50000/60000 (83%)]\tLoss: 13.264189\n",
      "====> Epoch: 37 Average loss: 13.5130\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 14.296392\n",
      "Train Epoch: 38 [10000/60000 (17%)]\tLoss: 12.528821\n",
      "Train Epoch: 38 [20000/60000 (33%)]\tLoss: 13.516648\n",
      "Train Epoch: 38 [30000/60000 (50%)]\tLoss: 12.649034\n",
      "Train Epoch: 38 [40000/60000 (67%)]\tLoss: 12.277397\n",
      "Train Epoch: 38 [50000/60000 (83%)]\tLoss: 15.435135\n",
      "====> Epoch: 38 Average loss: 13.5822\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 13.282300\n",
      "Train Epoch: 39 [10000/60000 (17%)]\tLoss: 13.285253\n",
      "Train Epoch: 39 [20000/60000 (33%)]\tLoss: 14.953501\n",
      "Train Epoch: 39 [30000/60000 (50%)]\tLoss: 13.689999\n",
      "Train Epoch: 39 [40000/60000 (67%)]\tLoss: 12.101221\n",
      "Train Epoch: 39 [50000/60000 (83%)]\tLoss: 12.796193\n",
      "====> Epoch: 39 Average loss: 13.5321\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 13.934449\n",
      "Train Epoch: 40 [10000/60000 (17%)]\tLoss: 11.823580\n",
      "Train Epoch: 40 [20000/60000 (33%)]\tLoss: 14.290176\n",
      "Train Epoch: 40 [30000/60000 (50%)]\tLoss: 12.822413\n",
      "Train Epoch: 40 [40000/60000 (67%)]\tLoss: 11.749354\n",
      "Train Epoch: 40 [50000/60000 (83%)]\tLoss: 12.257130\n",
      "====> Epoch: 40 Average loss: 13.5401\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 14.906613\n",
      "Train Epoch: 41 [10000/60000 (17%)]\tLoss: 12.205225\n",
      "Train Epoch: 41 [20000/60000 (33%)]\tLoss: 14.107303\n",
      "Train Epoch: 41 [30000/60000 (50%)]\tLoss: 13.880151\n",
      "Train Epoch: 41 [40000/60000 (67%)]\tLoss: 11.542345\n",
      "Train Epoch: 41 [50000/60000 (83%)]\tLoss: 12.041823\n",
      "Epoch   189: reducing learning rate of group 0 to 9.7656e-07.\n",
      "====> Epoch: 41 Average loss: 13.6255\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 13.075471\n",
      "Train Epoch: 42 [10000/60000 (17%)]\tLoss: 14.362225\n",
      "Train Epoch: 42 [20000/60000 (33%)]\tLoss: 15.592822\n",
      "Train Epoch: 42 [30000/60000 (50%)]\tLoss: 15.636930\n",
      "Train Epoch: 42 [40000/60000 (67%)]\tLoss: 17.740009\n",
      "Train Epoch: 42 [50000/60000 (83%)]\tLoss: 11.369000\n",
      "====> Epoch: 42 Average loss: 13.6147\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 11.937808\n",
      "Train Epoch: 43 [10000/60000 (17%)]\tLoss: 14.208397\n",
      "Train Epoch: 43 [20000/60000 (33%)]\tLoss: 12.992778\n",
      "Train Epoch: 43 [30000/60000 (50%)]\tLoss: 14.114117\n",
      "Train Epoch: 43 [40000/60000 (67%)]\tLoss: 15.581405\n",
      "Train Epoch: 43 [50000/60000 (83%)]\tLoss: 14.713118\n",
      "====> Epoch: 43 Average loss: 13.5787\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 12.490658\n",
      "Train Epoch: 44 [10000/60000 (17%)]\tLoss: 14.544100\n",
      "Train Epoch: 44 [20000/60000 (33%)]\tLoss: 12.758759\n",
      "Train Epoch: 44 [30000/60000 (50%)]\tLoss: 13.469812\n",
      "Train Epoch: 44 [40000/60000 (67%)]\tLoss: 12.863757\n",
      "Train Epoch: 44 [50000/60000 (83%)]\tLoss: 13.965637\n",
      "====> Epoch: 44 Average loss: 13.6023\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 13.420540\n",
      "Train Epoch: 45 [10000/60000 (17%)]\tLoss: 13.071063\n",
      "Train Epoch: 45 [20000/60000 (33%)]\tLoss: 16.651526\n",
      "Train Epoch: 45 [30000/60000 (50%)]\tLoss: 11.542316\n",
      "Train Epoch: 45 [40000/60000 (67%)]\tLoss: 16.259551\n",
      "Train Epoch: 45 [50000/60000 (83%)]\tLoss: 11.684935\n",
      "====> Epoch: 45 Average loss: 13.6041\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 10.995930\n",
      "Train Epoch: 46 [10000/60000 (17%)]\tLoss: 11.479315\n",
      "Train Epoch: 46 [20000/60000 (33%)]\tLoss: 13.516870\n",
      "Train Epoch: 46 [30000/60000 (50%)]\tLoss: 13.241234\n",
      "Train Epoch: 46 [40000/60000 (67%)]\tLoss: 12.864781\n",
      "Train Epoch: 46 [50000/60000 (83%)]\tLoss: 12.942852\n",
      "====> Epoch: 46 Average loss: 13.6465\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 10.666051\n",
      "Train Epoch: 47 [10000/60000 (17%)]\tLoss: 16.208260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 47 [20000/60000 (33%)]\tLoss: 16.795977\n",
      "Train Epoch: 47 [30000/60000 (50%)]\tLoss: 13.374729\n",
      "Train Epoch: 47 [40000/60000 (67%)]\tLoss: 15.040315\n",
      "Train Epoch: 47 [50000/60000 (83%)]\tLoss: 10.921901\n",
      "====> Epoch: 47 Average loss: 13.6582\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 14.077231\n",
      "Train Epoch: 48 [10000/60000 (17%)]\tLoss: 15.605826\n",
      "Train Epoch: 48 [20000/60000 (33%)]\tLoss: 13.814122\n",
      "Train Epoch: 48 [30000/60000 (50%)]\tLoss: 14.631218\n",
      "Train Epoch: 48 [40000/60000 (67%)]\tLoss: 12.555405\n",
      "Train Epoch: 48 [50000/60000 (83%)]\tLoss: 15.489392\n",
      "====> Epoch: 48 Average loss: 13.6679\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 14.766917\n",
      "Train Epoch: 49 [10000/60000 (17%)]\tLoss: 11.587874\n",
      "Train Epoch: 49 [20000/60000 (33%)]\tLoss: 14.434821\n",
      "Train Epoch: 49 [30000/60000 (50%)]\tLoss: 15.021462\n",
      "Train Epoch: 49 [40000/60000 (67%)]\tLoss: 14.385530\n",
      "Train Epoch: 49 [50000/60000 (83%)]\tLoss: 15.176610\n",
      "====> Epoch: 49 Average loss: 13.5462\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 15.726266\n",
      "Train Epoch: 50 [10000/60000 (17%)]\tLoss: 11.934821\n",
      "Train Epoch: 50 [20000/60000 (33%)]\tLoss: 11.629865\n",
      "Train Epoch: 50 [30000/60000 (50%)]\tLoss: 17.947830\n",
      "Train Epoch: 50 [40000/60000 (67%)]\tLoss: 13.347445\n",
      "Train Epoch: 50 [50000/60000 (83%)]\tLoss: 12.770652\n",
      "====> Epoch: 50 Average loss: 13.5880\n",
      "Train Epoch: 51 [0/60000 (0%)]\tLoss: 12.321245\n",
      "Train Epoch: 51 [10000/60000 (17%)]\tLoss: 14.102091\n",
      "Train Epoch: 51 [20000/60000 (33%)]\tLoss: 14.082981\n",
      "Train Epoch: 51 [30000/60000 (50%)]\tLoss: 12.165969\n",
      "Train Epoch: 51 [40000/60000 (67%)]\tLoss: 14.177798\n",
      "Train Epoch: 51 [50000/60000 (83%)]\tLoss: 16.868812\n",
      "====> Epoch: 51 Average loss: 13.6004\n",
      "Train Epoch: 52 [0/60000 (0%)]\tLoss: 14.169937\n",
      "Train Epoch: 52 [10000/60000 (17%)]\tLoss: 14.100051\n",
      "Train Epoch: 52 [20000/60000 (33%)]\tLoss: 13.910414\n",
      "Train Epoch: 52 [30000/60000 (50%)]\tLoss: 11.505354\n",
      "Train Epoch: 52 [40000/60000 (67%)]\tLoss: 11.791890\n",
      "Train Epoch: 52 [50000/60000 (83%)]\tLoss: 13.160415\n",
      "Epoch   200: reducing learning rate of group 0 to 4.8828e-07.\n",
      "====> Epoch: 52 Average loss: 13.5515\n",
      "Train Epoch: 53 [0/60000 (0%)]\tLoss: 13.691588\n",
      "Train Epoch: 53 [10000/60000 (17%)]\tLoss: 13.522531\n",
      "Train Epoch: 53 [20000/60000 (33%)]\tLoss: 10.752102\n",
      "Train Epoch: 53 [30000/60000 (50%)]\tLoss: 12.354924\n",
      "Train Epoch: 53 [40000/60000 (67%)]\tLoss: 13.827163\n",
      "Train Epoch: 53 [50000/60000 (83%)]\tLoss: 14.264498\n",
      "====> Epoch: 53 Average loss: 13.6047\n",
      "Train Epoch: 54 [0/60000 (0%)]\tLoss: 12.551315\n",
      "Train Epoch: 54 [10000/60000 (17%)]\tLoss: 12.086182\n",
      "Train Epoch: 54 [20000/60000 (33%)]\tLoss: 14.341368\n",
      "Train Epoch: 54 [30000/60000 (50%)]\tLoss: 13.473610\n",
      "Train Epoch: 54 [40000/60000 (67%)]\tLoss: 12.337563\n",
      "Train Epoch: 54 [50000/60000 (83%)]\tLoss: 14.280800\n",
      "====> Epoch: 54 Average loss: 13.6065\n",
      "Train Epoch: 55 [0/60000 (0%)]\tLoss: 14.181334\n",
      "Train Epoch: 55 [10000/60000 (17%)]\tLoss: 13.156776\n",
      "Train Epoch: 55 [20000/60000 (33%)]\tLoss: 12.892720\n",
      "Train Epoch: 55 [30000/60000 (50%)]\tLoss: 13.868396\n",
      "Train Epoch: 55 [40000/60000 (67%)]\tLoss: 14.262334\n",
      "Train Epoch: 55 [50000/60000 (83%)]\tLoss: 11.668203\n",
      "====> Epoch: 55 Average loss: 13.6075\n",
      "Train Epoch: 56 [0/60000 (0%)]\tLoss: 13.700316\n",
      "Train Epoch: 56 [10000/60000 (17%)]\tLoss: 10.703038\n",
      "Train Epoch: 56 [20000/60000 (33%)]\tLoss: 12.387688\n",
      "Train Epoch: 56 [30000/60000 (50%)]\tLoss: 16.338680\n",
      "Train Epoch: 56 [40000/60000 (67%)]\tLoss: 11.229738\n",
      "Train Epoch: 56 [50000/60000 (83%)]\tLoss: 13.586851\n",
      "====> Epoch: 56 Average loss: 13.5237\n",
      "Train Epoch: 57 [0/60000 (0%)]\tLoss: 11.301249\n",
      "Train Epoch: 57 [10000/60000 (17%)]\tLoss: 12.432552\n",
      "Train Epoch: 57 [20000/60000 (33%)]\tLoss: 10.374750\n",
      "Train Epoch: 57 [30000/60000 (50%)]\tLoss: 14.377852\n",
      "Train Epoch: 57 [40000/60000 (67%)]\tLoss: 16.332222\n",
      "Train Epoch: 57 [50000/60000 (83%)]\tLoss: 12.507655\n",
      "====> Epoch: 57 Average loss: 13.5277\n",
      "Train Epoch: 58 [0/60000 (0%)]\tLoss: 16.066329\n",
      "Train Epoch: 58 [10000/60000 (17%)]\tLoss: 13.433438\n",
      "Train Epoch: 58 [20000/60000 (33%)]\tLoss: 11.079843\n",
      "Train Epoch: 58 [30000/60000 (50%)]\tLoss: 12.992188\n",
      "Train Epoch: 58 [40000/60000 (67%)]\tLoss: 11.846334\n",
      "Train Epoch: 58 [50000/60000 (83%)]\tLoss: 13.407361\n",
      "====> Epoch: 58 Average loss: 13.6851\n",
      "Train Epoch: 59 [0/60000 (0%)]\tLoss: 13.101687\n",
      "Train Epoch: 59 [10000/60000 (17%)]\tLoss: 12.643169\n",
      "Train Epoch: 59 [20000/60000 (33%)]\tLoss: 15.922944\n",
      "Train Epoch: 59 [30000/60000 (50%)]\tLoss: 12.532708\n",
      "Train Epoch: 59 [40000/60000 (67%)]\tLoss: 12.520322\n",
      "Train Epoch: 59 [50000/60000 (83%)]\tLoss: 13.916865\n",
      "====> Epoch: 59 Average loss: 13.6180\n",
      "Train Epoch: 60 [0/60000 (0%)]\tLoss: 15.095145\n",
      "Train Epoch: 60 [10000/60000 (17%)]\tLoss: 13.073771\n",
      "Train Epoch: 60 [20000/60000 (33%)]\tLoss: 11.373237\n",
      "Train Epoch: 60 [30000/60000 (50%)]\tLoss: 15.125330\n",
      "Train Epoch: 60 [40000/60000 (67%)]\tLoss: 13.883700\n",
      "Train Epoch: 60 [50000/60000 (83%)]\tLoss: 13.063810\n",
      "====> Epoch: 60 Average loss: 13.5526\n",
      "Train Epoch: 61 [0/60000 (0%)]\tLoss: 11.212670\n",
      "Train Epoch: 61 [10000/60000 (17%)]\tLoss: 15.256047\n",
      "Train Epoch: 61 [20000/60000 (33%)]\tLoss: 15.443918\n",
      "Train Epoch: 61 [30000/60000 (50%)]\tLoss: 13.880811\n",
      "Train Epoch: 61 [40000/60000 (67%)]\tLoss: 13.579260\n",
      "Train Epoch: 61 [50000/60000 (83%)]\tLoss: 14.900950\n",
      "====> Epoch: 61 Average loss: 13.5731\n",
      "Train Epoch: 62 [0/60000 (0%)]\tLoss: 12.010001\n",
      "Train Epoch: 62 [10000/60000 (17%)]\tLoss: 12.945195\n",
      "Train Epoch: 62 [20000/60000 (33%)]\tLoss: 14.950947\n",
      "Train Epoch: 62 [30000/60000 (50%)]\tLoss: 15.831133\n",
      "Train Epoch: 62 [40000/60000 (67%)]\tLoss: 13.715403\n",
      "Train Epoch: 62 [50000/60000 (83%)]\tLoss: 13.241649\n",
      "====> Epoch: 62 Average loss: 13.5742\n",
      "Train Epoch: 63 [0/60000 (0%)]\tLoss: 11.049689\n",
      "Train Epoch: 63 [10000/60000 (17%)]\tLoss: 15.734038\n",
      "Train Epoch: 63 [20000/60000 (33%)]\tLoss: 12.722679\n",
      "Train Epoch: 63 [30000/60000 (50%)]\tLoss: 13.185479\n",
      "Train Epoch: 63 [40000/60000 (67%)]\tLoss: 11.963732\n",
      "Train Epoch: 63 [50000/60000 (83%)]\tLoss: 13.444369\n",
      "Epoch   211: reducing learning rate of group 0 to 2.4414e-07.\n",
      "====> Epoch: 63 Average loss: 13.6006\n",
      "Train Epoch: 64 [0/60000 (0%)]\tLoss: 14.249973\n",
      "Train Epoch: 64 [10000/60000 (17%)]\tLoss: 14.588837\n",
      "Train Epoch: 64 [20000/60000 (33%)]\tLoss: 12.328595\n",
      "Train Epoch: 64 [30000/60000 (50%)]\tLoss: 12.955544\n",
      "Train Epoch: 64 [40000/60000 (67%)]\tLoss: 14.512877\n",
      "Train Epoch: 64 [50000/60000 (83%)]\tLoss: 16.132507\n",
      "====> Epoch: 64 Average loss: 13.5771\n",
      "Train Epoch: 65 [0/60000 (0%)]\tLoss: 13.237603\n",
      "Train Epoch: 65 [10000/60000 (17%)]\tLoss: 13.737330\n",
      "Train Epoch: 65 [20000/60000 (33%)]\tLoss: 12.673831\n",
      "Train Epoch: 65 [30000/60000 (50%)]\tLoss: 13.138700\n",
      "Train Epoch: 65 [40000/60000 (67%)]\tLoss: 13.002102\n",
      "Train Epoch: 65 [50000/60000 (83%)]\tLoss: 11.704325\n",
      "====> Epoch: 65 Average loss: 13.6631\n",
      "Train Epoch: 66 [0/60000 (0%)]\tLoss: 10.275524\n",
      "Train Epoch: 66 [10000/60000 (17%)]\tLoss: 15.663745\n",
      "Train Epoch: 66 [20000/60000 (33%)]\tLoss: 15.138572\n",
      "Train Epoch: 66 [30000/60000 (50%)]\tLoss: 14.527212\n",
      "Train Epoch: 66 [40000/60000 (67%)]\tLoss: 13.943866\n",
      "Train Epoch: 66 [50000/60000 (83%)]\tLoss: 12.835360\n",
      "====> Epoch: 66 Average loss: 13.5780\n",
      "Train Epoch: 67 [0/60000 (0%)]\tLoss: 16.129928\n",
      "Train Epoch: 67 [10000/60000 (17%)]\tLoss: 14.513284\n",
      "Train Epoch: 67 [20000/60000 (33%)]\tLoss: 13.721791\n",
      "Train Epoch: 67 [30000/60000 (50%)]\tLoss: 14.777345\n",
      "Train Epoch: 67 [40000/60000 (67%)]\tLoss: 14.330525\n",
      "Train Epoch: 67 [50000/60000 (83%)]\tLoss: 12.072867\n",
      "====> Epoch: 67 Average loss: 13.5933\n",
      "Train Epoch: 68 [0/60000 (0%)]\tLoss: 12.693497\n",
      "Train Epoch: 68 [10000/60000 (17%)]\tLoss: 12.583774\n",
      "Train Epoch: 68 [20000/60000 (33%)]\tLoss: 14.153298\n",
      "Train Epoch: 68 [30000/60000 (50%)]\tLoss: 16.112288\n",
      "Train Epoch: 68 [40000/60000 (67%)]\tLoss: 16.417676\n",
      "Train Epoch: 68 [50000/60000 (83%)]\tLoss: 11.225198\n",
      "====> Epoch: 68 Average loss: 13.6660\n",
      "Train Epoch: 69 [0/60000 (0%)]\tLoss: 11.766676\n",
      "Train Epoch: 69 [10000/60000 (17%)]\tLoss: 16.369866\n",
      "Train Epoch: 69 [20000/60000 (33%)]\tLoss: 13.832916\n",
      "Train Epoch: 69 [30000/60000 (50%)]\tLoss: 15.753318\n",
      "Train Epoch: 69 [40000/60000 (67%)]\tLoss: 10.595862\n",
      "Train Epoch: 69 [50000/60000 (83%)]\tLoss: 13.897603\n",
      "====> Epoch: 69 Average loss: 13.5278\n",
      "Train Epoch: 70 [0/60000 (0%)]\tLoss: 13.127556\n",
      "Train Epoch: 70 [10000/60000 (17%)]\tLoss: 14.316543\n",
      "Train Epoch: 70 [20000/60000 (33%)]\tLoss: 14.648081\n",
      "Train Epoch: 70 [30000/60000 (50%)]\tLoss: 13.100059\n",
      "Train Epoch: 70 [40000/60000 (67%)]\tLoss: 15.058467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 70 [50000/60000 (83%)]\tLoss: 14.133949\n",
      "====> Epoch: 70 Average loss: 13.5441\n",
      "Train Epoch: 71 [0/60000 (0%)]\tLoss: 11.397399\n",
      "Train Epoch: 71 [10000/60000 (17%)]\tLoss: 12.639307\n",
      "Train Epoch: 71 [20000/60000 (33%)]\tLoss: 17.939556\n",
      "Train Epoch: 71 [30000/60000 (50%)]\tLoss: 15.057751\n",
      "Train Epoch: 71 [40000/60000 (67%)]\tLoss: 14.727836\n",
      "Train Epoch: 71 [50000/60000 (83%)]\tLoss: 13.282832\n",
      "====> Epoch: 71 Average loss: 13.5879\n",
      "Train Epoch: 72 [0/60000 (0%)]\tLoss: 15.839944\n",
      "Train Epoch: 72 [10000/60000 (17%)]\tLoss: 14.991002\n",
      "Train Epoch: 72 [20000/60000 (33%)]\tLoss: 14.428735\n",
      "Train Epoch: 72 [30000/60000 (50%)]\tLoss: 14.000206\n",
      "Train Epoch: 72 [40000/60000 (67%)]\tLoss: 13.533815\n",
      "Train Epoch: 72 [50000/60000 (83%)]\tLoss: 14.643307\n",
      "====> Epoch: 72 Average loss: 13.6121\n",
      "Train Epoch: 73 [0/60000 (0%)]\tLoss: 14.012208\n",
      "Train Epoch: 73 [10000/60000 (17%)]\tLoss: 15.364650\n",
      "Train Epoch: 73 [20000/60000 (33%)]\tLoss: 13.072472\n",
      "Train Epoch: 73 [30000/60000 (50%)]\tLoss: 10.817014\n",
      "Train Epoch: 73 [40000/60000 (67%)]\tLoss: 12.515325\n",
      "Train Epoch: 73 [50000/60000 (83%)]\tLoss: 15.556746\n",
      "====> Epoch: 73 Average loss: 13.6628\n",
      "Train Epoch: 74 [0/60000 (0%)]\tLoss: 17.212920\n",
      "Train Epoch: 74 [10000/60000 (17%)]\tLoss: 12.759666\n",
      "Train Epoch: 74 [20000/60000 (33%)]\tLoss: 12.828379\n",
      "Train Epoch: 74 [30000/60000 (50%)]\tLoss: 11.091439\n",
      "Train Epoch: 74 [40000/60000 (67%)]\tLoss: 14.215771\n",
      "Train Epoch: 74 [50000/60000 (83%)]\tLoss: 14.213398\n",
      "Epoch   222: reducing learning rate of group 0 to 1.2207e-07.\n",
      "====> Epoch: 74 Average loss: 13.5688\n",
      "Train Epoch: 75 [0/60000 (0%)]\tLoss: 17.112223\n",
      "Train Epoch: 75 [10000/60000 (17%)]\tLoss: 11.398553\n",
      "Train Epoch: 75 [20000/60000 (33%)]\tLoss: 13.448687\n",
      "Train Epoch: 75 [30000/60000 (50%)]\tLoss: 13.302178\n",
      "Train Epoch: 75 [40000/60000 (67%)]\tLoss: 11.375195\n",
      "Train Epoch: 75 [50000/60000 (83%)]\tLoss: 13.270314\n",
      "====> Epoch: 75 Average loss: 13.6274\n",
      "Train Epoch: 76 [0/60000 (0%)]\tLoss: 14.248812\n",
      "Train Epoch: 76 [10000/60000 (17%)]\tLoss: 13.216561\n",
      "Train Epoch: 76 [20000/60000 (33%)]\tLoss: 11.602517\n",
      "Train Epoch: 76 [30000/60000 (50%)]\tLoss: 13.262568\n",
      "Train Epoch: 76 [40000/60000 (67%)]\tLoss: 13.006663\n",
      "Train Epoch: 76 [50000/60000 (83%)]\tLoss: 13.595634\n",
      "====> Epoch: 76 Average loss: 13.5520\n",
      "Train Epoch: 77 [0/60000 (0%)]\tLoss: 13.600026\n",
      "Train Epoch: 77 [10000/60000 (17%)]\tLoss: 15.713810\n",
      "Train Epoch: 77 [20000/60000 (33%)]\tLoss: 14.733647\n",
      "Train Epoch: 77 [30000/60000 (50%)]\tLoss: 13.597594\n",
      "Train Epoch: 77 [40000/60000 (67%)]\tLoss: 13.248801\n",
      "Train Epoch: 77 [50000/60000 (83%)]\tLoss: 12.528970\n",
      "====> Epoch: 77 Average loss: 13.6196\n",
      "Train Epoch: 78 [0/60000 (0%)]\tLoss: 14.074276\n",
      "Train Epoch: 78 [10000/60000 (17%)]\tLoss: 9.346058\n",
      "Train Epoch: 78 [20000/60000 (33%)]\tLoss: 13.059840\n",
      "Train Epoch: 78 [30000/60000 (50%)]\tLoss: 13.177360\n",
      "Train Epoch: 78 [40000/60000 (67%)]\tLoss: 12.575852\n",
      "Train Epoch: 78 [50000/60000 (83%)]\tLoss: 12.106650\n",
      "====> Epoch: 78 Average loss: 13.6275\n",
      "Train Epoch: 79 [0/60000 (0%)]\tLoss: 15.069723\n",
      "Train Epoch: 79 [10000/60000 (17%)]\tLoss: 10.003464\n",
      "Train Epoch: 79 [20000/60000 (33%)]\tLoss: 11.717335\n",
      "Train Epoch: 79 [30000/60000 (50%)]\tLoss: 15.058447\n",
      "Train Epoch: 79 [40000/60000 (67%)]\tLoss: 11.526632\n",
      "Train Epoch: 79 [50000/60000 (83%)]\tLoss: 12.639653\n",
      "====> Epoch: 79 Average loss: 13.5822\n",
      "Train Epoch: 80 [0/60000 (0%)]\tLoss: 13.256664\n",
      "Train Epoch: 80 [10000/60000 (17%)]\tLoss: 12.714578\n",
      "Train Epoch: 80 [20000/60000 (33%)]\tLoss: 14.825112\n",
      "Train Epoch: 80 [30000/60000 (50%)]\tLoss: 12.943813\n",
      "Train Epoch: 80 [40000/60000 (67%)]\tLoss: 13.988507\n",
      "Train Epoch: 80 [50000/60000 (83%)]\tLoss: 12.714255\n",
      "====> Epoch: 80 Average loss: 13.5301\n",
      "Train Epoch: 81 [0/60000 (0%)]\tLoss: 10.699730\n",
      "Train Epoch: 81 [10000/60000 (17%)]\tLoss: 14.216958\n",
      "Train Epoch: 81 [20000/60000 (33%)]\tLoss: 16.011659\n",
      "Train Epoch: 81 [30000/60000 (50%)]\tLoss: 13.268145\n",
      "Train Epoch: 81 [40000/60000 (67%)]\tLoss: 13.652673\n",
      "Train Epoch: 81 [50000/60000 (83%)]\tLoss: 14.142482\n",
      "====> Epoch: 81 Average loss: 13.5634\n",
      "Train Epoch: 82 [0/60000 (0%)]\tLoss: 13.028038\n",
      "Train Epoch: 82 [10000/60000 (17%)]\tLoss: 14.572612\n",
      "Train Epoch: 82 [20000/60000 (33%)]\tLoss: 11.536127\n",
      "Train Epoch: 82 [30000/60000 (50%)]\tLoss: 13.938889\n",
      "Train Epoch: 82 [40000/60000 (67%)]\tLoss: 15.960892\n",
      "Train Epoch: 82 [50000/60000 (83%)]\tLoss: 16.077756\n",
      "====> Epoch: 82 Average loss: 13.6013\n",
      "Train Epoch: 83 [0/60000 (0%)]\tLoss: 12.330880\n",
      "Train Epoch: 83 [10000/60000 (17%)]\tLoss: 13.337817\n",
      "Train Epoch: 83 [20000/60000 (33%)]\tLoss: 14.033486\n",
      "Train Epoch: 83 [30000/60000 (50%)]\tLoss: 12.172833\n",
      "Train Epoch: 83 [40000/60000 (67%)]\tLoss: 14.308054\n",
      "Train Epoch: 83 [50000/60000 (83%)]\tLoss: 13.656174\n",
      "====> Epoch: 83 Average loss: 13.6657\n",
      "Train Epoch: 84 [0/60000 (0%)]\tLoss: 13.825035\n",
      "Train Epoch: 84 [10000/60000 (17%)]\tLoss: 13.721241\n",
      "Train Epoch: 84 [20000/60000 (33%)]\tLoss: 12.132489\n",
      "Train Epoch: 84 [30000/60000 (50%)]\tLoss: 17.068210\n",
      "Train Epoch: 84 [40000/60000 (67%)]\tLoss: 11.967106\n",
      "Train Epoch: 84 [50000/60000 (83%)]\tLoss: 16.322576\n",
      "====> Epoch: 84 Average loss: 13.6488\n",
      "Train Epoch: 85 [0/60000 (0%)]\tLoss: 14.962299\n",
      "Train Epoch: 85 [10000/60000 (17%)]\tLoss: 13.134948\n",
      "Train Epoch: 85 [20000/60000 (33%)]\tLoss: 16.398853\n",
      "Train Epoch: 85 [30000/60000 (50%)]\tLoss: 11.263235\n",
      "Train Epoch: 85 [40000/60000 (67%)]\tLoss: 12.527559\n",
      "Train Epoch: 85 [50000/60000 (83%)]\tLoss: 14.053776\n",
      "Epoch   233: reducing learning rate of group 0 to 6.1035e-08.\n",
      "====> Epoch: 85 Average loss: 13.5676\n",
      "Train Epoch: 86 [0/60000 (0%)]\tLoss: 13.514670\n",
      "Train Epoch: 86 [10000/60000 (17%)]\tLoss: 13.616414\n",
      "Train Epoch: 86 [20000/60000 (33%)]\tLoss: 12.332585\n",
      "Train Epoch: 86 [30000/60000 (50%)]\tLoss: 12.488414\n",
      "Train Epoch: 86 [40000/60000 (67%)]\tLoss: 13.190952\n",
      "Train Epoch: 86 [50000/60000 (83%)]\tLoss: 15.820085\n",
      "====> Epoch: 86 Average loss: 13.5037\n",
      "Train Epoch: 87 [0/60000 (0%)]\tLoss: 13.213677\n",
      "Train Epoch: 87 [10000/60000 (17%)]\tLoss: 13.259174\n",
      "Train Epoch: 87 [20000/60000 (33%)]\tLoss: 14.096058\n",
      "Train Epoch: 87 [30000/60000 (50%)]\tLoss: 14.562734\n",
      "Train Epoch: 87 [40000/60000 (67%)]\tLoss: 12.217698\n",
      "Train Epoch: 87 [50000/60000 (83%)]\tLoss: 14.991288\n",
      "====> Epoch: 87 Average loss: 13.5516\n",
      "Train Epoch: 88 [0/60000 (0%)]\tLoss: 13.556702\n",
      "Train Epoch: 88 [10000/60000 (17%)]\tLoss: 15.427035\n",
      "Train Epoch: 88 [20000/60000 (33%)]\tLoss: 12.625031\n",
      "Train Epoch: 88 [30000/60000 (50%)]\tLoss: 13.863405\n",
      "Train Epoch: 88 [40000/60000 (67%)]\tLoss: 13.655148\n",
      "Train Epoch: 88 [50000/60000 (83%)]\tLoss: 13.726198\n",
      "====> Epoch: 88 Average loss: 13.5205\n",
      "Train Epoch: 89 [0/60000 (0%)]\tLoss: 16.055394\n",
      "Train Epoch: 89 [10000/60000 (17%)]\tLoss: 14.368801\n",
      "Train Epoch: 89 [20000/60000 (33%)]\tLoss: 14.489432\n",
      "Train Epoch: 89 [30000/60000 (50%)]\tLoss: 12.318645\n",
      "Train Epoch: 89 [40000/60000 (67%)]\tLoss: 14.323456\n",
      "Train Epoch: 89 [50000/60000 (83%)]\tLoss: 12.195408\n",
      "====> Epoch: 89 Average loss: 13.4930\n",
      "Train Epoch: 90 [0/60000 (0%)]\tLoss: 12.433282\n",
      "Train Epoch: 90 [10000/60000 (17%)]\tLoss: 12.898241\n",
      "Train Epoch: 90 [20000/60000 (33%)]\tLoss: 13.917406\n",
      "Train Epoch: 90 [30000/60000 (50%)]\tLoss: 14.203315\n",
      "Train Epoch: 90 [40000/60000 (67%)]\tLoss: 13.734338\n",
      "Train Epoch: 90 [50000/60000 (83%)]\tLoss: 15.256372\n",
      "====> Epoch: 90 Average loss: 13.5835\n",
      "Train Epoch: 91 [0/60000 (0%)]\tLoss: 12.176573\n",
      "Train Epoch: 91 [10000/60000 (17%)]\tLoss: 11.990615\n",
      "Train Epoch: 91 [20000/60000 (33%)]\tLoss: 11.359823\n",
      "Train Epoch: 91 [30000/60000 (50%)]\tLoss: 12.215894\n",
      "Train Epoch: 91 [40000/60000 (67%)]\tLoss: 12.380894\n",
      "Train Epoch: 91 [50000/60000 (83%)]\tLoss: 15.453639\n",
      "====> Epoch: 91 Average loss: 13.5110\n",
      "Train Epoch: 92 [0/60000 (0%)]\tLoss: 14.293400\n",
      "Train Epoch: 92 [10000/60000 (17%)]\tLoss: 13.028346\n",
      "Train Epoch: 92 [20000/60000 (33%)]\tLoss: 12.182743\n",
      "Train Epoch: 92 [30000/60000 (50%)]\tLoss: 14.015811\n",
      "Train Epoch: 92 [40000/60000 (67%)]\tLoss: 12.381124\n",
      "Train Epoch: 92 [50000/60000 (83%)]\tLoss: 12.603569\n",
      "====> Epoch: 92 Average loss: 13.5703\n",
      "Train Epoch: 93 [0/60000 (0%)]\tLoss: 12.121296\n",
      "Train Epoch: 93 [10000/60000 (17%)]\tLoss: 13.535199\n",
      "Train Epoch: 93 [20000/60000 (33%)]\tLoss: 12.873370\n",
      "Train Epoch: 93 [30000/60000 (50%)]\tLoss: 13.693773\n",
      "Train Epoch: 93 [40000/60000 (67%)]\tLoss: 15.542103\n",
      "Train Epoch: 93 [50000/60000 (83%)]\tLoss: 14.035712\n",
      "====> Epoch: 93 Average loss: 13.6084\n",
      "Train Epoch: 94 [0/60000 (0%)]\tLoss: 12.939744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 94 [10000/60000 (17%)]\tLoss: 14.533945\n",
      "Train Epoch: 94 [20000/60000 (33%)]\tLoss: 16.982859\n",
      "Train Epoch: 94 [30000/60000 (50%)]\tLoss: 13.000333\n",
      "Train Epoch: 94 [40000/60000 (67%)]\tLoss: 13.461346\n",
      "Train Epoch: 94 [50000/60000 (83%)]\tLoss: 12.383329\n",
      "====> Epoch: 94 Average loss: 13.5381\n",
      "Train Epoch: 95 [0/60000 (0%)]\tLoss: 12.559562\n",
      "Train Epoch: 95 [10000/60000 (17%)]\tLoss: 13.084889\n",
      "Train Epoch: 95 [20000/60000 (33%)]\tLoss: 13.339564\n",
      "Train Epoch: 95 [30000/60000 (50%)]\tLoss: 15.475558\n",
      "Train Epoch: 95 [40000/60000 (67%)]\tLoss: 12.670946\n",
      "Train Epoch: 95 [50000/60000 (83%)]\tLoss: 14.497698\n",
      "====> Epoch: 95 Average loss: 13.6529\n",
      "Train Epoch: 96 [0/60000 (0%)]\tLoss: 15.611093\n",
      "Train Epoch: 96 [10000/60000 (17%)]\tLoss: 12.300853\n",
      "Train Epoch: 96 [20000/60000 (33%)]\tLoss: 10.878754\n",
      "Train Epoch: 96 [30000/60000 (50%)]\tLoss: 16.945924\n",
      "Train Epoch: 96 [40000/60000 (67%)]\tLoss: 13.733262\n",
      "Train Epoch: 96 [50000/60000 (83%)]\tLoss: 11.180181\n",
      "Epoch   244: reducing learning rate of group 0 to 3.0518e-08.\n",
      "====> Epoch: 96 Average loss: 13.6821\n",
      "Train Epoch: 97 [0/60000 (0%)]\tLoss: 11.508232\n",
      "Train Epoch: 97 [10000/60000 (17%)]\tLoss: 14.951880\n",
      "Train Epoch: 97 [20000/60000 (33%)]\tLoss: 12.426840\n",
      "Train Epoch: 97 [30000/60000 (50%)]\tLoss: 11.517826\n",
      "Train Epoch: 97 [40000/60000 (67%)]\tLoss: 12.598932\n",
      "Train Epoch: 97 [50000/60000 (83%)]\tLoss: 14.877603\n",
      "====> Epoch: 97 Average loss: 13.5916\n",
      "Train Epoch: 98 [0/60000 (0%)]\tLoss: 12.473940\n",
      "Train Epoch: 98 [10000/60000 (17%)]\tLoss: 16.850126\n",
      "Train Epoch: 98 [20000/60000 (33%)]\tLoss: 12.874998\n",
      "Train Epoch: 98 [30000/60000 (50%)]\tLoss: 13.120671\n",
      "Train Epoch: 98 [40000/60000 (67%)]\tLoss: 15.800865\n",
      "Train Epoch: 98 [50000/60000 (83%)]\tLoss: 13.608674\n",
      "====> Epoch: 98 Average loss: 13.6517\n",
      "Train Epoch: 99 [0/60000 (0%)]\tLoss: 15.689338\n",
      "Train Epoch: 99 [10000/60000 (17%)]\tLoss: 12.469402\n",
      "Train Epoch: 99 [20000/60000 (33%)]\tLoss: 12.122797\n",
      "Train Epoch: 99 [30000/60000 (50%)]\tLoss: 11.624045\n",
      "Train Epoch: 99 [40000/60000 (67%)]\tLoss: 12.282791\n",
      "Train Epoch: 99 [50000/60000 (83%)]\tLoss: 13.588027\n",
      "====> Epoch: 99 Average loss: 13.6300\n",
      "Train Epoch: 100 [0/60000 (0%)]\tLoss: 14.676616\n",
      "Train Epoch: 100 [10000/60000 (17%)]\tLoss: 12.482455\n",
      "Train Epoch: 100 [20000/60000 (33%)]\tLoss: 13.874269\n",
      "Train Epoch: 100 [30000/60000 (50%)]\tLoss: 10.983256\n",
      "Train Epoch: 100 [40000/60000 (67%)]\tLoss: 14.248145\n",
      "Train Epoch: 100 [50000/60000 (83%)]\tLoss: 11.631682\n",
      "====> Epoch: 100 Average loss: 13.5952\n",
      "Train Epoch: 101 [0/60000 (0%)]\tLoss: 10.610980\n",
      "Train Epoch: 101 [10000/60000 (17%)]\tLoss: 14.105757\n",
      "Train Epoch: 101 [20000/60000 (33%)]\tLoss: 11.255825\n",
      "Train Epoch: 101 [30000/60000 (50%)]\tLoss: 15.996569\n",
      "Train Epoch: 101 [40000/60000 (67%)]\tLoss: 11.331353\n",
      "Train Epoch: 101 [50000/60000 (83%)]\tLoss: 12.212310\n",
      "====> Epoch: 101 Average loss: 13.5110\n",
      "Train Epoch: 102 [0/60000 (0%)]\tLoss: 14.576216\n",
      "Train Epoch: 102 [10000/60000 (17%)]\tLoss: 10.968604\n",
      "Train Epoch: 102 [20000/60000 (33%)]\tLoss: 11.984872\n",
      "Train Epoch: 102 [30000/60000 (50%)]\tLoss: 15.260305\n",
      "Train Epoch: 102 [40000/60000 (67%)]\tLoss: 15.853953\n",
      "Train Epoch: 102 [50000/60000 (83%)]\tLoss: 13.303698\n",
      "====> Epoch: 102 Average loss: 13.5458\n",
      "Train Epoch: 103 [0/60000 (0%)]\tLoss: 12.989547\n",
      "Train Epoch: 103 [10000/60000 (17%)]\tLoss: 12.752333\n",
      "Train Epoch: 103 [20000/60000 (33%)]\tLoss: 10.743219\n",
      "Train Epoch: 103 [30000/60000 (50%)]\tLoss: 14.422694\n",
      "Train Epoch: 103 [40000/60000 (67%)]\tLoss: 14.068580\n",
      "Train Epoch: 103 [50000/60000 (83%)]\tLoss: 11.866490\n",
      "====> Epoch: 103 Average loss: 13.6430\n",
      "Train Epoch: 104 [0/60000 (0%)]\tLoss: 13.934714\n",
      "Train Epoch: 104 [10000/60000 (17%)]\tLoss: 13.892981\n",
      "Train Epoch: 104 [20000/60000 (33%)]\tLoss: 14.730275\n",
      "Train Epoch: 104 [30000/60000 (50%)]\tLoss: 15.161328\n",
      "Train Epoch: 104 [40000/60000 (67%)]\tLoss: 18.571163\n",
      "Train Epoch: 104 [50000/60000 (83%)]\tLoss: 11.447675\n",
      "====> Epoch: 104 Average loss: 13.5967\n",
      "Train Epoch: 105 [0/60000 (0%)]\tLoss: 12.804618\n",
      "Train Epoch: 105 [10000/60000 (17%)]\tLoss: 14.309353\n",
      "Train Epoch: 105 [20000/60000 (33%)]\tLoss: 12.846816\n",
      "Train Epoch: 105 [30000/60000 (50%)]\tLoss: 13.219432\n",
      "Train Epoch: 105 [40000/60000 (67%)]\tLoss: 13.049484\n",
      "Train Epoch: 105 [50000/60000 (83%)]\tLoss: 14.425122\n",
      "====> Epoch: 105 Average loss: 13.5385\n",
      "Train Epoch: 106 [0/60000 (0%)]\tLoss: 11.951292\n",
      "Train Epoch: 106 [10000/60000 (17%)]\tLoss: 12.016298\n",
      "Train Epoch: 106 [20000/60000 (33%)]\tLoss: 14.026080\n",
      "Train Epoch: 106 [30000/60000 (50%)]\tLoss: 15.981208\n",
      "Train Epoch: 106 [40000/60000 (67%)]\tLoss: 13.621007\n",
      "Train Epoch: 106 [50000/60000 (83%)]\tLoss: 14.388080\n",
      "====> Epoch: 106 Average loss: 13.5796\n",
      "Train Epoch: 107 [0/60000 (0%)]\tLoss: 16.804476\n",
      "Train Epoch: 107 [10000/60000 (17%)]\tLoss: 13.822557\n",
      "Train Epoch: 107 [20000/60000 (33%)]\tLoss: 13.385646\n",
      "Train Epoch: 107 [30000/60000 (50%)]\tLoss: 14.571534\n",
      "Train Epoch: 107 [40000/60000 (67%)]\tLoss: 16.123419\n",
      "Train Epoch: 107 [50000/60000 (83%)]\tLoss: 12.946522\n",
      "Epoch   255: reducing learning rate of group 0 to 1.5259e-08.\n",
      "====> Epoch: 107 Average loss: 13.7268\n",
      "Train Epoch: 108 [0/60000 (0%)]\tLoss: 14.057906\n",
      "Train Epoch: 108 [10000/60000 (17%)]\tLoss: 13.733547\n",
      "Train Epoch: 108 [20000/60000 (33%)]\tLoss: 16.612849\n",
      "Train Epoch: 108 [30000/60000 (50%)]\tLoss: 14.527146\n",
      "Train Epoch: 108 [40000/60000 (67%)]\tLoss: 14.664121\n",
      "Train Epoch: 108 [50000/60000 (83%)]\tLoss: 16.210206\n",
      "====> Epoch: 108 Average loss: 13.6856\n",
      "Train Epoch: 109 [0/60000 (0%)]\tLoss: 13.861340\n",
      "Train Epoch: 109 [10000/60000 (17%)]\tLoss: 14.470702\n",
      "Train Epoch: 109 [20000/60000 (33%)]\tLoss: 11.898199\n",
      "Train Epoch: 109 [30000/60000 (50%)]\tLoss: 12.648115\n",
      "Train Epoch: 109 [40000/60000 (67%)]\tLoss: 9.723305\n",
      "Train Epoch: 109 [50000/60000 (83%)]\tLoss: 13.210061\n",
      "====> Epoch: 109 Average loss: 13.5165\n",
      "Train Epoch: 110 [0/60000 (0%)]\tLoss: 13.195353\n",
      "Train Epoch: 110 [10000/60000 (17%)]\tLoss: 12.082288\n",
      "Train Epoch: 110 [20000/60000 (33%)]\tLoss: 14.522864\n",
      "Train Epoch: 110 [30000/60000 (50%)]\tLoss: 12.748951\n",
      "Train Epoch: 110 [40000/60000 (67%)]\tLoss: 12.771774\n",
      "Train Epoch: 110 [50000/60000 (83%)]\tLoss: 15.142090\n",
      "====> Epoch: 110 Average loss: 13.6072\n",
      "Train Epoch: 111 [0/60000 (0%)]\tLoss: 14.281068\n",
      "Train Epoch: 111 [10000/60000 (17%)]\tLoss: 13.069006\n",
      "Train Epoch: 111 [20000/60000 (33%)]\tLoss: 12.896968\n",
      "Train Epoch: 111 [30000/60000 (50%)]\tLoss: 13.935139\n",
      "Train Epoch: 111 [40000/60000 (67%)]\tLoss: 13.972617\n",
      "Train Epoch: 111 [50000/60000 (83%)]\tLoss: 12.568392\n",
      "====> Epoch: 111 Average loss: 13.6026\n",
      "Train Epoch: 112 [0/60000 (0%)]\tLoss: 13.346514\n",
      "Train Epoch: 112 [10000/60000 (17%)]\tLoss: 15.258573\n",
      "Train Epoch: 112 [20000/60000 (33%)]\tLoss: 12.412242\n",
      "Train Epoch: 112 [30000/60000 (50%)]\tLoss: 14.611783\n",
      "Train Epoch: 112 [40000/60000 (67%)]\tLoss: 13.387742\n",
      "Train Epoch: 112 [50000/60000 (83%)]\tLoss: 12.759977\n",
      "====> Epoch: 112 Average loss: 13.5789\n",
      "Train Epoch: 113 [0/60000 (0%)]\tLoss: 12.225178\n",
      "Train Epoch: 113 [10000/60000 (17%)]\tLoss: 12.056252\n",
      "Train Epoch: 113 [20000/60000 (33%)]\tLoss: 11.082949\n",
      "Train Epoch: 113 [30000/60000 (50%)]\tLoss: 14.209218\n",
      "Train Epoch: 113 [40000/60000 (67%)]\tLoss: 11.766583\n",
      "Train Epoch: 113 [50000/60000 (83%)]\tLoss: 11.950052\n",
      "====> Epoch: 113 Average loss: 13.6123\n",
      "Train Epoch: 114 [0/60000 (0%)]\tLoss: 15.281195\n",
      "Train Epoch: 114 [10000/60000 (17%)]\tLoss: 16.448441\n",
      "Train Epoch: 114 [20000/60000 (33%)]\tLoss: 11.583047\n",
      "Train Epoch: 114 [30000/60000 (50%)]\tLoss: 16.488014\n",
      "Train Epoch: 114 [40000/60000 (67%)]\tLoss: 13.495758\n",
      "Train Epoch: 114 [50000/60000 (83%)]\tLoss: 13.060485\n",
      "====> Epoch: 114 Average loss: 13.5300\n",
      "Train Epoch: 115 [0/60000 (0%)]\tLoss: 13.901290\n",
      "Train Epoch: 115 [10000/60000 (17%)]\tLoss: 14.553101\n",
      "Train Epoch: 115 [20000/60000 (33%)]\tLoss: 13.524221\n",
      "Train Epoch: 115 [30000/60000 (50%)]\tLoss: 11.950553\n",
      "Train Epoch: 115 [40000/60000 (67%)]\tLoss: 14.232313\n",
      "Train Epoch: 115 [50000/60000 (83%)]\tLoss: 13.128810\n",
      "====> Epoch: 115 Average loss: 13.5836\n",
      "Train Epoch: 116 [0/60000 (0%)]\tLoss: 11.961752\n",
      "Train Epoch: 116 [10000/60000 (17%)]\tLoss: 14.824263\n",
      "Train Epoch: 116 [20000/60000 (33%)]\tLoss: 13.597163\n",
      "Train Epoch: 116 [30000/60000 (50%)]\tLoss: 16.060262\n",
      "Train Epoch: 116 [40000/60000 (67%)]\tLoss: 13.524130\n",
      "Train Epoch: 116 [50000/60000 (83%)]\tLoss: 13.529724\n",
      "====> Epoch: 116 Average loss: 13.5680\n",
      "Train Epoch: 117 [0/60000 (0%)]\tLoss: 15.667853\n",
      "Train Epoch: 117 [10000/60000 (17%)]\tLoss: 11.918630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 117 [20000/60000 (33%)]\tLoss: 14.365479\n",
      "Train Epoch: 117 [30000/60000 (50%)]\tLoss: 12.143236\n",
      "Train Epoch: 117 [40000/60000 (67%)]\tLoss: 14.227109\n",
      "Train Epoch: 117 [50000/60000 (83%)]\tLoss: 10.369169\n",
      "====> Epoch: 117 Average loss: 13.4861\n",
      "Train Epoch: 118 [0/60000 (0%)]\tLoss: 17.464093\n",
      "Train Epoch: 118 [10000/60000 (17%)]\tLoss: 11.531973\n",
      "Train Epoch: 118 [20000/60000 (33%)]\tLoss: 13.298359\n",
      "Train Epoch: 118 [30000/60000 (50%)]\tLoss: 15.368831\n",
      "Train Epoch: 118 [40000/60000 (67%)]\tLoss: 13.984597\n",
      "Train Epoch: 118 [50000/60000 (83%)]\tLoss: 15.178926\n",
      "====> Epoch: 118 Average loss: 13.5228\n",
      "Train Epoch: 119 [0/60000 (0%)]\tLoss: 14.416622\n",
      "Train Epoch: 119 [10000/60000 (17%)]\tLoss: 10.826089\n",
      "Train Epoch: 119 [20000/60000 (33%)]\tLoss: 14.157717\n",
      "Train Epoch: 119 [30000/60000 (50%)]\tLoss: 11.508241\n",
      "Train Epoch: 119 [40000/60000 (67%)]\tLoss: 15.657960\n",
      "Train Epoch: 119 [50000/60000 (83%)]\tLoss: 14.055966\n",
      "====> Epoch: 119 Average loss: 13.5364\n",
      "Train Epoch: 120 [0/60000 (0%)]\tLoss: 15.500042\n",
      "Train Epoch: 120 [10000/60000 (17%)]\tLoss: 13.496143\n",
      "Train Epoch: 120 [20000/60000 (33%)]\tLoss: 14.359127\n",
      "Train Epoch: 120 [30000/60000 (50%)]\tLoss: 14.609972\n",
      "Train Epoch: 120 [40000/60000 (67%)]\tLoss: 15.086133\n",
      "Train Epoch: 120 [50000/60000 (83%)]\tLoss: 13.599489\n",
      "====> Epoch: 120 Average loss: 13.6371\n",
      "Train Epoch: 121 [0/60000 (0%)]\tLoss: 14.938011\n",
      "Train Epoch: 121 [10000/60000 (17%)]\tLoss: 12.873290\n",
      "Train Epoch: 121 [20000/60000 (33%)]\tLoss: 12.414596\n",
      "Train Epoch: 121 [30000/60000 (50%)]\tLoss: 13.246038\n",
      "Train Epoch: 121 [40000/60000 (67%)]\tLoss: 11.212263\n",
      "Train Epoch: 121 [50000/60000 (83%)]\tLoss: 12.614298\n",
      "====> Epoch: 121 Average loss: 13.6376\n",
      "Train Epoch: 122 [0/60000 (0%)]\tLoss: 10.530586\n",
      "Train Epoch: 122 [10000/60000 (17%)]\tLoss: 15.811761\n",
      "Train Epoch: 122 [20000/60000 (33%)]\tLoss: 12.658378\n",
      "Train Epoch: 122 [30000/60000 (50%)]\tLoss: 11.710007\n",
      "Train Epoch: 122 [40000/60000 (67%)]\tLoss: 14.589294\n",
      "Train Epoch: 122 [50000/60000 (83%)]\tLoss: 13.747485\n",
      "====> Epoch: 122 Average loss: 13.5368\n",
      "Train Epoch: 123 [0/60000 (0%)]\tLoss: 10.974133\n",
      "Train Epoch: 123 [10000/60000 (17%)]\tLoss: 12.614520\n",
      "Train Epoch: 123 [20000/60000 (33%)]\tLoss: 12.188340\n",
      "Train Epoch: 123 [30000/60000 (50%)]\tLoss: 18.621649\n",
      "Train Epoch: 123 [40000/60000 (67%)]\tLoss: 14.435480\n",
      "Train Epoch: 123 [50000/60000 (83%)]\tLoss: 14.993970\n",
      "====> Epoch: 123 Average loss: 13.6456\n",
      "Train Epoch: 124 [0/60000 (0%)]\tLoss: 12.038802\n",
      "Train Epoch: 124 [10000/60000 (17%)]\tLoss: 12.053027\n",
      "Train Epoch: 124 [20000/60000 (33%)]\tLoss: 12.577731\n",
      "Train Epoch: 124 [30000/60000 (50%)]\tLoss: 15.753855\n",
      "Train Epoch: 124 [40000/60000 (67%)]\tLoss: 14.435492\n",
      "Train Epoch: 124 [50000/60000 (83%)]\tLoss: 12.268843\n",
      "====> Epoch: 124 Average loss: 13.6212\n",
      "Train Epoch: 125 [0/60000 (0%)]\tLoss: 13.116743\n",
      "Train Epoch: 125 [10000/60000 (17%)]\tLoss: 13.688461\n",
      "Train Epoch: 125 [20000/60000 (33%)]\tLoss: 12.708179\n",
      "Train Epoch: 125 [30000/60000 (50%)]\tLoss: 13.568540\n",
      "Train Epoch: 125 [40000/60000 (67%)]\tLoss: 13.908628\n",
      "Train Epoch: 125 [50000/60000 (83%)]\tLoss: 11.302109\n",
      "====> Epoch: 125 Average loss: 13.5552\n",
      "Train Epoch: 126 [0/60000 (0%)]\tLoss: 14.425040\n",
      "Train Epoch: 126 [10000/60000 (17%)]\tLoss: 11.858071\n",
      "Train Epoch: 126 [20000/60000 (33%)]\tLoss: 11.997021\n",
      "Train Epoch: 126 [30000/60000 (50%)]\tLoss: 12.168619\n",
      "Train Epoch: 126 [40000/60000 (67%)]\tLoss: 13.227590\n",
      "Train Epoch: 126 [50000/60000 (83%)]\tLoss: 13.576868\n",
      "====> Epoch: 126 Average loss: 13.6714\n",
      "Train Epoch: 127 [0/60000 (0%)]\tLoss: 13.346304\n",
      "Train Epoch: 127 [10000/60000 (17%)]\tLoss: 12.146819\n",
      "Train Epoch: 127 [20000/60000 (33%)]\tLoss: 13.598286\n",
      "Train Epoch: 127 [30000/60000 (50%)]\tLoss: 12.935499\n",
      "Train Epoch: 127 [40000/60000 (67%)]\tLoss: 13.815343\n",
      "Train Epoch: 127 [50000/60000 (83%)]\tLoss: 11.782979\n",
      "====> Epoch: 127 Average loss: 13.4861\n",
      "Train Epoch: 128 [0/60000 (0%)]\tLoss: 14.835862\n",
      "Train Epoch: 128 [10000/60000 (17%)]\tLoss: 11.447350\n",
      "Train Epoch: 128 [20000/60000 (33%)]\tLoss: 14.202175\n",
      "Train Epoch: 128 [30000/60000 (50%)]\tLoss: 14.525957\n",
      "Train Epoch: 128 [40000/60000 (67%)]\tLoss: 12.929279\n",
      "Train Epoch: 128 [50000/60000 (83%)]\tLoss: 12.100985\n",
      "====> Epoch: 128 Average loss: 13.5944\n",
      "Train Epoch: 129 [0/60000 (0%)]\tLoss: 12.771660\n",
      "Train Epoch: 129 [10000/60000 (17%)]\tLoss: 15.841066\n",
      "Train Epoch: 129 [20000/60000 (33%)]\tLoss: 15.802875\n",
      "Train Epoch: 129 [30000/60000 (50%)]\tLoss: 15.884863\n",
      "Train Epoch: 129 [40000/60000 (67%)]\tLoss: 15.465017\n",
      "Train Epoch: 129 [50000/60000 (83%)]\tLoss: 13.845555\n",
      "====> Epoch: 129 Average loss: 13.5632\n",
      "Train Epoch: 130 [0/60000 (0%)]\tLoss: 12.319296\n",
      "Train Epoch: 130 [10000/60000 (17%)]\tLoss: 16.468008\n",
      "Train Epoch: 130 [20000/60000 (33%)]\tLoss: 12.376190\n",
      "Train Epoch: 130 [30000/60000 (50%)]\tLoss: 15.134850\n",
      "Train Epoch: 130 [40000/60000 (67%)]\tLoss: 13.136794\n",
      "Train Epoch: 130 [50000/60000 (83%)]\tLoss: 11.564741\n",
      "====> Epoch: 130 Average loss: 13.5972\n",
      "Train Epoch: 131 [0/60000 (0%)]\tLoss: 13.270667\n",
      "Train Epoch: 131 [10000/60000 (17%)]\tLoss: 14.615007\n",
      "Train Epoch: 131 [20000/60000 (33%)]\tLoss: 11.390117\n",
      "Train Epoch: 131 [30000/60000 (50%)]\tLoss: 14.646041\n",
      "Train Epoch: 131 [40000/60000 (67%)]\tLoss: 15.228555\n",
      "Train Epoch: 131 [50000/60000 (83%)]\tLoss: 15.333706\n",
      "====> Epoch: 131 Average loss: 13.5322\n",
      "Train Epoch: 132 [0/60000 (0%)]\tLoss: 13.817593\n",
      "Train Epoch: 132 [10000/60000 (17%)]\tLoss: 11.211287\n",
      "Train Epoch: 132 [20000/60000 (33%)]\tLoss: 17.104994\n",
      "Train Epoch: 132 [30000/60000 (50%)]\tLoss: 15.314775\n",
      "Train Epoch: 132 [40000/60000 (67%)]\tLoss: 13.092347\n",
      "Train Epoch: 132 [50000/60000 (83%)]\tLoss: 17.488818\n",
      "====> Epoch: 132 Average loss: 13.6344\n",
      "Train Epoch: 133 [0/60000 (0%)]\tLoss: 15.515063\n",
      "Train Epoch: 133 [10000/60000 (17%)]\tLoss: 14.513412\n",
      "Train Epoch: 133 [20000/60000 (33%)]\tLoss: 13.227144\n",
      "Train Epoch: 133 [30000/60000 (50%)]\tLoss: 12.486721\n",
      "Train Epoch: 133 [40000/60000 (67%)]\tLoss: 10.764000\n",
      "Train Epoch: 133 [50000/60000 (83%)]\tLoss: 14.456013\n",
      "====> Epoch: 133 Average loss: 13.5635\n",
      "Train Epoch: 134 [0/60000 (0%)]\tLoss: 12.532429\n",
      "Train Epoch: 134 [10000/60000 (17%)]\tLoss: 14.609259\n",
      "Train Epoch: 134 [20000/60000 (33%)]\tLoss: 11.732644\n",
      "Train Epoch: 134 [30000/60000 (50%)]\tLoss: 13.295626\n",
      "Train Epoch: 134 [40000/60000 (67%)]\tLoss: 11.860024\n",
      "Train Epoch: 134 [50000/60000 (83%)]\tLoss: 11.759635\n",
      "====> Epoch: 134 Average loss: 13.5186\n",
      "Train Epoch: 135 [0/60000 (0%)]\tLoss: 13.750288\n",
      "Train Epoch: 135 [10000/60000 (17%)]\tLoss: 15.796600\n",
      "Train Epoch: 135 [20000/60000 (33%)]\tLoss: 13.083131\n",
      "Train Epoch: 135 [30000/60000 (50%)]\tLoss: 12.871782\n",
      "Train Epoch: 135 [40000/60000 (67%)]\tLoss: 14.464358\n",
      "Train Epoch: 135 [50000/60000 (83%)]\tLoss: 12.722703\n",
      "====> Epoch: 135 Average loss: 13.5575\n",
      "Train Epoch: 136 [0/60000 (0%)]\tLoss: 11.456829\n",
      "Train Epoch: 136 [10000/60000 (17%)]\tLoss: 12.743085\n",
      "Train Epoch: 136 [20000/60000 (33%)]\tLoss: 11.624835\n",
      "Train Epoch: 136 [30000/60000 (50%)]\tLoss: 13.508094\n",
      "Train Epoch: 136 [40000/60000 (67%)]\tLoss: 11.883160\n",
      "Train Epoch: 136 [50000/60000 (83%)]\tLoss: 14.673033\n",
      "====> Epoch: 136 Average loss: 13.5848\n",
      "Train Epoch: 137 [0/60000 (0%)]\tLoss: 15.111049\n",
      "Train Epoch: 137 [10000/60000 (17%)]\tLoss: 15.579729\n",
      "Train Epoch: 137 [20000/60000 (33%)]\tLoss: 12.525077\n",
      "Train Epoch: 137 [30000/60000 (50%)]\tLoss: 11.683330\n",
      "Train Epoch: 137 [40000/60000 (67%)]\tLoss: 14.315787\n",
      "Train Epoch: 137 [50000/60000 (83%)]\tLoss: 11.352341\n",
      "====> Epoch: 137 Average loss: 13.5556\n",
      "Train Epoch: 138 [0/60000 (0%)]\tLoss: 14.300928\n",
      "Train Epoch: 138 [10000/60000 (17%)]\tLoss: 13.276058\n",
      "Train Epoch: 138 [20000/60000 (33%)]\tLoss: 12.517727\n",
      "Train Epoch: 138 [30000/60000 (50%)]\tLoss: 12.359299\n",
      "Train Epoch: 138 [40000/60000 (67%)]\tLoss: 13.091357\n",
      "Train Epoch: 138 [50000/60000 (83%)]\tLoss: 13.538257\n",
      "====> Epoch: 138 Average loss: 13.6047\n",
      "Train Epoch: 139 [0/60000 (0%)]\tLoss: 13.889636\n",
      "Train Epoch: 139 [10000/60000 (17%)]\tLoss: 12.251504\n",
      "Train Epoch: 139 [20000/60000 (33%)]\tLoss: 14.052274\n",
      "Train Epoch: 139 [30000/60000 (50%)]\tLoss: 14.046855\n",
      "Train Epoch: 139 [40000/60000 (67%)]\tLoss: 10.634343\n",
      "Train Epoch: 139 [50000/60000 (83%)]\tLoss: 12.650551\n",
      "====> Epoch: 139 Average loss: 13.6143\n",
      "Train Epoch: 140 [0/60000 (0%)]\tLoss: 13.359727\n",
      "Train Epoch: 140 [10000/60000 (17%)]\tLoss: 11.359391\n",
      "Train Epoch: 140 [20000/60000 (33%)]\tLoss: 16.149208\n",
      "Train Epoch: 140 [30000/60000 (50%)]\tLoss: 11.031769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 140 [40000/60000 (67%)]\tLoss: 13.846749\n",
      "Train Epoch: 140 [50000/60000 (83%)]\tLoss: 11.627144\n",
      "====> Epoch: 140 Average loss: 13.4537\n",
      "Train Epoch: 141 [0/60000 (0%)]\tLoss: 12.152332\n",
      "Train Epoch: 141 [10000/60000 (17%)]\tLoss: 15.695801\n",
      "Train Epoch: 141 [20000/60000 (33%)]\tLoss: 13.456658\n",
      "Train Epoch: 141 [30000/60000 (50%)]\tLoss: 16.195100\n",
      "Train Epoch: 141 [40000/60000 (67%)]\tLoss: 13.325427\n",
      "Train Epoch: 141 [50000/60000 (83%)]\tLoss: 11.825596\n",
      "====> Epoch: 141 Average loss: 13.6076\n",
      "Train Epoch: 142 [0/60000 (0%)]\tLoss: 13.763912\n",
      "Train Epoch: 142 [10000/60000 (17%)]\tLoss: 15.552341\n",
      "Train Epoch: 142 [20000/60000 (33%)]\tLoss: 12.529021\n",
      "Train Epoch: 142 [30000/60000 (50%)]\tLoss: 15.079213\n",
      "Train Epoch: 142 [40000/60000 (67%)]\tLoss: 12.997487\n",
      "Train Epoch: 142 [50000/60000 (83%)]\tLoss: 15.149089\n",
      "====> Epoch: 142 Average loss: 13.7688\n",
      "Train Epoch: 143 [0/60000 (0%)]\tLoss: 13.467197\n",
      "Train Epoch: 143 [10000/60000 (17%)]\tLoss: 14.743806\n",
      "Train Epoch: 143 [20000/60000 (33%)]\tLoss: 14.542288\n",
      "Train Epoch: 143 [30000/60000 (50%)]\tLoss: 13.043765\n",
      "Train Epoch: 143 [40000/60000 (67%)]\tLoss: 13.912773\n",
      "Train Epoch: 143 [50000/60000 (83%)]\tLoss: 13.310945\n",
      "====> Epoch: 143 Average loss: 13.5463\n",
      "Train Epoch: 144 [0/60000 (0%)]\tLoss: 16.789562\n",
      "Train Epoch: 144 [10000/60000 (17%)]\tLoss: 12.862516\n",
      "Train Epoch: 144 [20000/60000 (33%)]\tLoss: 11.864906\n",
      "Train Epoch: 144 [30000/60000 (50%)]\tLoss: 15.478062\n",
      "Train Epoch: 144 [40000/60000 (67%)]\tLoss: 12.192300\n",
      "Train Epoch: 144 [50000/60000 (83%)]\tLoss: 13.902054\n",
      "====> Epoch: 144 Average loss: 13.5685\n",
      "Train Epoch: 145 [0/60000 (0%)]\tLoss: 14.450527\n",
      "Train Epoch: 145 [10000/60000 (17%)]\tLoss: 14.345503\n",
      "Train Epoch: 145 [20000/60000 (33%)]\tLoss: 14.067399\n",
      "Train Epoch: 145 [30000/60000 (50%)]\tLoss: 13.089508\n",
      "Train Epoch: 145 [40000/60000 (67%)]\tLoss: 17.594232\n",
      "Train Epoch: 145 [50000/60000 (83%)]\tLoss: 11.803074\n",
      "====> Epoch: 145 Average loss: 13.6614\n",
      "Train Epoch: 146 [0/60000 (0%)]\tLoss: 15.447539\n",
      "Train Epoch: 146 [10000/60000 (17%)]\tLoss: 12.792689\n",
      "Train Epoch: 146 [20000/60000 (33%)]\tLoss: 11.744558\n",
      "Train Epoch: 146 [30000/60000 (50%)]\tLoss: 13.458665\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-d1f5b370852b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-fcd43a3d2f16>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/UMNN/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/UMNN/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/UMNN/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/UMNN/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/UMNN/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/UMNN/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    train(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(10).float()/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:23: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:25: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:28: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:91: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:106: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:23: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:25: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:28: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:91: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:106: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-2a9a968977e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mthis_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from os.path import join\n",
    "from PIL import Image, ImageOps\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "this_root = os.path.abspath(os.path.dirname(__file__))\n",
    "\n",
    "\n",
    "def load_image(file_path, input_height=128, input_width=None, output_height=128, output_width=None,\n",
    "               crop_height=None, crop_width=None, is_random_crop=True, is_mirror=False, is_gray=False):\n",
    "    if input_width is None:\n",
    "        input_width = input_height\n",
    "    if output_width is None:\n",
    "        output_width = output_height\n",
    "    if crop_width is None:\n",
    "        crop_width = crop_height\n",
    "\n",
    "    img = Image.open(file_path)\n",
    "    if is_gray is False and img.mode is not 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    if is_gray and img.mode is not 'L':\n",
    "        img = img.convert('L')\n",
    "\n",
    "    if is_mirror and random.randint(0, 1) is 0:\n",
    "        img = ImageOps.mirror(img)\n",
    "\n",
    "    if input_height is not None:\n",
    "        img = img.resize((input_width, input_height), Image.BICUBIC)\n",
    "\n",
    "    if crop_height is not None:\n",
    "        [w, h] = img.size\n",
    "        if is_random_crop:\n",
    "            cx1 = random.randint(0, w - crop_width)\n",
    "            cx2 = w - crop_width - cx1\n",
    "            cy1 = random.randint(0, h - crop_height)\n",
    "            cy2 = h - crop_height - cy1\n",
    "        else:\n",
    "            cx2 = cx1 = int(round((w - crop_width) / 2.))\n",
    "            cy2 = cy1 = int(round((h - crop_height) / 2.))\n",
    "        img = ImageOps.crop(img, (cx1, cy1, cx2, cy2))\n",
    "\n",
    "    img = img.resize((output_height, output_width), Image.BICUBIC)\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_fake_image(img, input_height, input_width, output_height, output_width):\n",
    "    fake_image = torch.load(img)\n",
    "    return fake_image\n",
    "\n",
    "\n",
    "def get_list_filenames(root_path):\n",
    "    list = []\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            if not file.endswith(\".jpg\"):\n",
    "                continue\n",
    "            path = os.path.join(root, file).replace(root_path, '')\n",
    "            list.append(path)\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, root_path, filename='1000_fake_tensor_cifar_10', dataset_type='celeba', input_height=128,\n",
    "                 crop_height=None, crop_width=None, is_random_crop=False, is_mirror=True,\n",
    "                 is_gray=False):\n",
    "        \"\"\"\n",
    "        :param root_path: Path to the directory of the dataset\n",
    "        :param filename: Name of the file\n",
    "        :param dataset_type: Which dataset we are referring to\n",
    "        :param input_height: Height of the image. Default set to 128\n",
    "        :param crop_height:\n",
    "        :param crop_width:\n",
    "        :param is_random_crop:\n",
    "        :param is_mirror:\n",
    "        :param is_gray:\n",
    "        \"\"\"\n",
    "        super(Dataset, self).__init__()\n",
    "        self.dataset_type = dataset_type\n",
    "        self.root_path = root_path\n",
    "        self.input_height = input_height\n",
    "        self.is_random_crop = is_random_crop\n",
    "        self.is_mirror = is_mirror\n",
    "        self.crop_height = crop_height\n",
    "        self.crop_width = crop_width\n",
    "        self.filename = filename\n",
    "        self.is_gray = is_gray\n",
    "\n",
    "        if dataset_type is 'celeba':\n",
    "            self.image_filenames = get_list_filenames(root_path)\n",
    "\n",
    "            self.input_transform = transforms.Compose([\n",
    "\n",
    "                transforms.Resize([self.input_height, self.input_height]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "            # db = datasets.ImageFolder(root, transform=transform)\n",
    "            indice = list(range(0, 5000))\n",
    "            try_sampler = data.SubsetRandomSampler(indice)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.dataset_type is 'celeba':\n",
    "            img = load_image(join(self.root_path, self.image_filenames[index]),\n",
    "                             self.input_height, self.input_width, self.output_height, self.output_width,\n",
    "                             self.crop_height, self.crop_width, self.is_random_crop, self.is_mirror, self.is_gray)\n",
    "\n",
    "            img = self.input_transform(img)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainset = Dataset(this_root, dataset_type='fake_generated')\n",
    "    trainloader = data.DataLoader(trainset, batch_size=1, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "    # transform = transforms.Compose([ transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    # trainloader = data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "    # torch_file = os.path.join(this_root, '4_fake_tensor_cifar_10')\n",
    "    #\n",
    "    # fake = torch.load(torch_file)\n",
    "\n",
    "    # print(len(fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res_Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Res Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=64, out_channels=64, avg=False, upsample=False, ngpu=1):  # groups=1, scale=1.0\n",
    "        super(Res_Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        self.avg = avg\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "        self.upsample = upsample\n",
    "        # self.upsample_layer = nn.Upsample(scale_factor=2, mode='nearest') #was deprecated\n",
    "        self.upsample_layer = Interpolate(scale_factor=2, mode='nearest')\n",
    "        self.addon = nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False)\n",
    "        self.ngpu = ngpu\n",
    "        self.layers = [self.conv1, self.bn, self.relu, self.conv2]\n",
    "        if in_channels > out_channels:\n",
    "            self.sample = 1\n",
    "        elif in_channels == out_channels:\n",
    "            self.sample = 0\n",
    "        else:\n",
    "            self.sample = -1\n",
    "            if self.upsample:\n",
    "                self.layers = [self.upsample_layer, self.conv1, self.bn, self.relu, self.conv2]\n",
    "\n",
    "    def forward(self, input):  # for encoder and generator\n",
    "        if self.sample == 0:\n",
    "            if self.upsample:\n",
    "                input = self.upsample_layer(input)\n",
    "            residual = input\n",
    "            if self.ngpu == 0:\n",
    "                output = self.relu(self.bn(self.conv1(input)))\n",
    "                output = self.conv2(output)\n",
    "                output += residual\n",
    "                output = self.relu(self.bn(output))\n",
    "            else:\n",
    "                gpu_ids = range(self.ngpu)\n",
    "                self.net = nn.Sequential(*self.layers)\n",
    "                output = nn.parallel.data_parallel(self.net, input, gpu_ids)\n",
    "            if self.avg:\n",
    "                output = self.avgpool(output)\n",
    "\n",
    "\n",
    "        elif self.sample == -1:  # for encoder, out_ch should be in_ch * 2\n",
    "            identity = self.addon(input)\n",
    "            output = self.relu(self.bn(self.conv1(input)))\n",
    "            output = self.conv2(output)\n",
    "            output += identity\n",
    "            if self.avg == True:\n",
    "                output = self.avgpool(output)\n",
    "\n",
    "        else:  # for generator, out_ch should be in_ch/2\n",
    "            if self.upsample:\n",
    "                input = self.upsample_layer(input)\n",
    "            identity = self.addon(input)\n",
    "            output = self.relu(self.bn(self.conv1(input)))\n",
    "            output = self.conv2(output)\n",
    "            output += identity\n",
    "            output = self.relu(self.bn(output))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Interpolate(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper interpolate function\n",
    "    \"\"\"\n",
    "    def __init__(self, scale_factor, mode):\n",
    "        super(Interpolate, self).__init__()\n",
    "        self.interp = nn.functional.interpolate\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Intro_enc(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder model\n",
    "    \"\"\"\n",
    "    def __init__(self, num_col=3, img_dim=256, z_dim=512, ngpu=1):  # groups=1, scale=1.0\n",
    "        super(Intro_enc, self).__init__()\n",
    "        self.dim = img_dim\n",
    "        self.nc = num_col\n",
    "        self.c_dim = self.dim // 8\n",
    "        self.layers = [nn.Conv2d(self.nc, self.c_dim, 5, 1, 2, bias=False),\n",
    "                       nn.BatchNorm2d(self.c_dim),\n",
    "                       nn.LeakyReLU(0.2),\n",
    "                       nn.AvgPool2d(2)]\n",
    "        self.zdim = self.dim * 2\n",
    "        self.fc = nn.Linear(z_dim * 4 * 4, 2 * z_dim)\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        if self.dim == 256:  # 32, 64, 128, 256, 512, 512\n",
    "            # 32 * 128 * 128\n",
    "            self.layers.extend([Res_Block(32, 64, avg=True, ngpu=ngpu),  # 64 * 64 * 64\n",
    "                                Res_Block(64, 128, avg=True, ngpu=ngpu),  # 128 * 32 * 32\n",
    "                                Res_Block(128, 256, avg=True, ngpu=ngpu),  # 256 * 16 * 16\n",
    "                                Res_Block(256, 512, avg=True, ngpu=ngpu),  # 512 * 8 * 8\n",
    "                                Res_Block(512, 512, avg=True, ngpu=ngpu),\n",
    "                                Res_Block(512, 512, ngpu=ngpu)])  # 512 * 4 * 4\n",
    "\n",
    "        elif self.dim == 128:  # 16, 32, 64, 128, 256, 256\n",
    "            # I assume the channel sequence start from 16 for 128*128 image(as in 1024*1024)\n",
    "            # instead of 32 in 256*256, so that it can have similar number of Res-block\n",
    "            # (while 5 for 128*1286 for 256*256, 8 for 1024*1024)\n",
    "            # 16 * 64 * 64\n",
    "            '''\n",
    "            self.net.add_model('res64', Res_Block(16, 32, avg=True))# 32 * 32 * 32\n",
    "            self.net.add_model('res64', Res_Block(32, 64, avg=True))# 64 * 16 * 16\n",
    "            self.net.add_model('res128', Res_Block(64, 128, avg=True))# 128 * 8 * 8\n",
    "            self.net.add_model('res256', Res_Block(128, 256, avg=True))# 256 * 4 * 4\n",
    "            '''\n",
    "            self.layers.extend([\n",
    "                Res_Block(16, 32, avg=True, ngpu=ngpu),\n",
    "                Res_Block(32, 64, avg=True, ngpu=ngpu),\n",
    "                Res_Block(64, 128, avg=True, ngpu=ngpu),\n",
    "                Res_Block(128, 256, avg=True, ngpu=ngpu),\n",
    "                Res_Block(256, 256, ngpu=ngpu)\n",
    "            ])\n",
    "\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.ngpu == 0:\n",
    "            output = self.net(input)\n",
    "            output = output.view(output.size(0), -1)\n",
    "            output = self.fc(output)\n",
    "        else:\n",
    "            gpu_ids = range(self.ngpu)\n",
    "            output = nn.parallel.data_parallel(self.net, input, gpu_ids)\n",
    "            output = output.view(output.size(0), -1)  # reshape\n",
    "            output = nn.parallel.data_parallel(self.fc, output, gpu_ids)\n",
    "\n",
    "        mean, logvar = output.chunk(2, dim=1)  # although dunno why\n",
    "\n",
    "        return mean, logvar\n",
    "\n",
    "\n",
    "class Intro_gen(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator model\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dim=256, num_col=3, z_dim=512, ngpu=1):\n",
    "        super(Intro_gen, self).__init__()\n",
    "        self.dim = img_dim\n",
    "        self.nc = num_col\n",
    "        self.z_dim = z_dim\n",
    "        self.fc = nn.Linear(self.z_dim, self.z_dim * 4 * 4)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        if self.z_dim == 512:\n",
    "            self.layers = [\n",
    "                Res_Block(512, 512, ngpu=ngpu),\n",
    "                Res_Block(512, 512, upsample=True, ngpu=ngpu),\n",
    "                Res_Block(512, 256, upsample=True, ngpu=ngpu),\n",
    "                Res_Block(256, 128, upsample=True, ngpu=ngpu),\n",
    "                Res_Block(128, 64, upsample=True, ngpu=ngpu),\n",
    "                Res_Block(64, 32, upsample=True, ngpu=ngpu),\n",
    "                Res_Block(32, 32, upsample=True, ngpu=ngpu),\n",
    "                nn.Conv2d(32, num_col, 5, 1, 2)\n",
    "            ]\n",
    "\n",
    "        elif self.z_dim == 256:\n",
    "            self.layers = [\n",
    "                Res_Block(256, 256, ngpu=ngpu),\n",
    "                Res_Block(256, 128, upsample=True, ngpu=ngpu),\n",
    "                Res_Block(128, 64, upsample=True, ngpu=ngpu),\n",
    "                Res_Block(64, 32, upsample=True, ngpu=ngpu),\n",
    "                Res_Block(32, 16, upsample=True, ngpu=ngpu),\n",
    "                Res_Block(16, 16, upsample=True, ngpu=ngpu),\n",
    "                nn.Conv2d(16, num_col, 5, 1, 2)\n",
    "            ]\n",
    "\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input: latent vector\n",
    "        input = self.relu(self.fc(input))\n",
    "        input = input.view(-1, self.z_dim, 4, 4)\n",
    "        if self.ngpu == 0:\n",
    "            output = self.net(input)\n",
    "        else:\n",
    "            gpu_ids = range(self.ngpu)\n",
    "            output = nn.parallel.data_parallel(self.net, input, gpu_ids)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Intro_enc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_MAX = 25\n",
    "latent_s = 25\n",
    "t_emb_s = 1\n",
    "pos_enc = StupidPositionalEncoder(T_MAX)#PositionalEncoder(t_emb_s//2)#\n",
    "dev = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "dec = TemporalDecoder(784, latent_s, 256, t_emb_s).to(dev)\n",
    "enc = TemporalEncoder(784, latent_s, 256, t_emb_s).to(dev)\n",
    "trans = TransitionNet(latent_s, 100, t_emb_s).to(dev)\n",
    "dif = dataDiffuser(beta_min=1e-2, beta_max=1., t_max=T_MAX).to(dev)\n",
    "sampling_t0 = False\n",
    "(1 - dif.alphas).sqrt(), (dif.alphas).sqrt()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:UMNN]",
   "language": "python",
   "name": "conda-env-UMNN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
